{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca905cf-7dea-49c1-8371-22c25a5bf1fe",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\">\n",
    "    <img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\"\n",
    "         width=\"400\" height=\"186\" />\n",
    "</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9428c62-3f09-4439-a826-513d088ae146",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\">**Notebook 3:** Fine-tuning an LLM with NeMo Customizer MS</font>\n",
    "\n",
    "### Overview\n",
    "\n",
    "In Notebook 2, we explored techniques to improve LLM performance through zero-shot inference and in-context learning (ICL). While these methods showed promise, they have limitations:\n",
    "\n",
    "1. **Zero-Shot**: Base model performance may be insufficient for specialized domains\n",
    "2. **In-Context Learning**: Limited by context window size and doesn't permanently improve the model\n",
    "3. **Prompt Engineering**: Can be complex to maintain and may not scale well\n",
    "\n",
    "This notebook introduces Low-Rank Adaptation (LoRA) as a more permanent solution for domain adaptation. Unlike previous methods that work within the model's existing capabilities, LoRA actually modifies the model to better handle domain-specific tasks.\n",
    "\n",
    "### What is LoRA?\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is an efficient fine-tuning technique that:\n",
    "- Freezes the original model weights\n",
    "- Adds small trainable \"rank decomposition matrices\" to each layer\n",
    "- Dramatically reduces the number of trainable parameters (often by 99%)\n",
    "- Maintains model quality while being much more efficient\n",
    "\n",
    "For example, instead of training all 7 billion parameters in a model, LoRA might only train 10 million parameters while achieving similar or better performance than ICL.\n",
    "\n",
    "### Comparing Adaptation Approaches\n",
    "\n",
    "| Aspect | Zero-Shot | ICL (Notebook 2) | LoRA (This Notebook) |\n",
    "|--------|-----------|------------------|---------------------|\n",
    "| Setup Time | None | Minutes | Hours |\n",
    "| Compute Required | Minimal | Minimal | Moderate |\n",
    "| Performance | Baseline | Good | Better |\n",
    "| Permanence | N/A | Temporary | Permanent |\n",
    "| Memory Usage | Base | 2-3x Base | Base + Small Adapter |\n",
    "| Scalability | Limited | Context Limited | Highly Scalable |\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "In this notebook you will learn how to:\n",
    "- Prepare legal domain data for LoRA fine-tuning\n",
    "- Configure and launch training jobs with NeMo Customizer\n",
    "- Monitor training progress and analyze results\n",
    "- Deploy and test the fine-tuned model\n",
    "- Compare performance against previous approaches\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "2. [Install Python Package Requirements](#Install-Python-Package-Requirements)\n",
    "3. [Set up MLflow for Experiment Tracking](#Set-up-MLflow-for-Experiment-Tracking)\n",
    "4. [Configure NeMo Microservices Endpoints](#Set-up-NeMo-Microservice-API-endpoints)\n",
    "5. [Prepare Training Data](#Prepare-Training-Data)\n",
    "   * [Format Dataset](#Format-Dataset)\n",
    "   * [Upload to NeMo Data Store](#Upload-to-NeMo-Data-Store)\n",
    "6. [Configure LoRA Training](#Configure-LoRA-Training)\n",
    "   * [Set Hyperparameters](#Set-Hyperparameters)\n",
    "   * [Launch Training Job](#Launch-Training-Job)\n",
    "7. [Monitor and Evaluate](#Monitor-and-Evaluate)\n",
    "   * [Track Training Progress](#Track-Training-Progress)\n",
    "   * [Analyze Results](#Analyze-Results)\n",
    "8. [Deploy Fine-tuned Model](#Deploy-Fine-tuned-Model)\n",
    "   * [Load LoRA Adapter](#Load-LoRA-Adapter)\n",
    "   * [Compare Performance](#Compare-Performance)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "The following NeMo microservices must be running:\n",
    "\n",
    "| MS | Purpose | Namespace |\n",
    "|----|---------|-----------|\n",
    "| **NeMo Data Store** | Dataset & model artifacts | `datastore` |\n",
    "| **NeMo Entity Store** | Platform-wide entities | `entitystore` |\n",
    "| **NeMo Customizer** | Fine-tuning (LoRA) | `customizer` |\n",
    "| **NIM** (Llama 3.2-3B-instruct) | Base model endpoint | `llama3-2-3b-instruct` |\n",
    "\n",
    "> For deployment instructions, see the [NeMo Microservices documentation](https://developer.nvidia.com/docs/nemo-microservices/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d4194-5326-4850-883a-37886a28f95f",
   "metadata": {},
   "source": [
    "Set the following variables to the hostname of each microservice before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a8aee4-20a3-4f02-81a5-f89b2bc71c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_url = \"http://nemo-datastore.local\"\n",
    "nim_url = \"http://llama3-2-3b-instruct.local\"\n",
    "eval_url = \"http://nemo-evaluator.local\"\n",
    "customizer_url = \"http://nemo-customizer.local\"\n",
    "entitystore_url = \"http://nemo-entity-store.local\"\n",
    "\n",
    "NIM_model_id = \"meta/llama-3.2-3b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b70b45f-2eb0-42c6-986d-9383cc3f8366",
   "metadata": {},
   "source": [
    "We can verify the following services are running and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12961e-f483-4c78-9430-cdc51a1652c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "resp = requests.get(f\"{eval_url}/health\")\n",
    "print(resp.status_code)\n",
    "resp = requests.get(f\"{datastore_url}/v1/health\")\n",
    "print(resp.status_code)\n",
    "resp = requests.get(f\"{entitystore_url}/v1/health/ready\")\n",
    "print(resp.status_code)\n",
    "resp = requests.get(f\"{customizer_url}/health/ready\")\n",
    "print(resp.status_code)\n",
    "resp = requests.get(f\"{nim_url}/v1/health/ready\")\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b8d84-dfea-4cad-b691-a415b9a35984",
   "metadata": {},
   "source": [
    "We will use the dataset that we already created in notebook 2, so in this notebook we only need to define the parameters that were used to upload the dataset to the Nemo Data-store and Entity-store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb8133-d45f-4177-a9a1-d596e4124b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"legal_dataset_notebook_2\"\n",
    "repo_id = \"default/\" + dataset_name\n",
    "repo_type = \"dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bae050-9298-41d2-a3d8-ebf2615440e2",
   "metadata": {},
   "source": [
    "Let's first check the available configurations we currently have in the deployed Nemo Customizer MS; this can be modified to other models as described in the following [documentation](https://docs.nvidia.com/nemo/microservices/latest/fine-tune/models/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c1b132-123c-4e58-93e7-b866ac3bc864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "resp = requests.get(f\"{customizer_url}/v1/customization/configs\",\n",
    "                     headers=headers,\n",
    "                     timeout=30)\n",
    "\n",
    "print(json.dumps(resp.json(), indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c26f1e-1c01-4f28-93ae-faba65c1deda",
   "metadata": {},
   "source": [
    "We can now kickoff the customization job using the curated datasets, the hyperparameters used can be customized based on your needs, however, consider that adding more epochs will incur longer customization time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744f15c-8499-411c-97b5-e61ed2df6caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from pprint import pprint\n",
    "\n",
    "# assumes `customizer_url` and `dataset_name` are already defined earlier\n",
    "payload = {\n",
    "    \"config\": \"meta/llama-3.2-3b-instruct\",\n",
    "    \"dataset\": {\"name\": dataset_name},\n",
    "    \"hyperparameters\": {\n",
    "        \"training_type\": \"sft\",\n",
    "        \"finetuning_type\": \"lora\",\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"lora\": {\n",
    "            \"adapter_dim\": 8,\n",
    "            \"adapter_dropout\": 0.1,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{customizer_url}/v1/customization/jobs\",\n",
    "                     headers=headers,\n",
    "                     json=payload,\n",
    "                     timeout=30)\n",
    "\n",
    "print(\"HTTP\", resp.status_code)\n",
    "pprint(resp.json() if resp.headers.get(\"content-type\") == \"application/json\" else resp.text)\n",
    "cust_id = resp.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f5b9e1-1395-4d96-a332-caaf32007766",
   "metadata": {},
   "source": [
    "The customization proceess is expected to take 5-10 minutes. To monitor the progress of the customization job, we will check its status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29a313-211d-495d-a834-8393cda5d90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys\n",
    "from datetime import datetime\n",
    "\n",
    "status = \"initializing\"\n",
    "refresh_interval = 30  # seconds between refreshes\n",
    "\n",
    "while status in {\"initializing\", \"running\", \"created\"}:\n",
    "    job = requests.get(f\"{customizer_url}/v1/customization/jobs/{cust_id}\",\n",
    "                      timeout=10, verify=False).json()\n",
    "    status = job[\"status\"]\n",
    "    \n",
    "    # Show status with countdown timer\n",
    "    for remaining in range(refresh_interval, 0, -1):\n",
    "        sys.stdout.write('\\033[K')  # Clear line\n",
    "        sys.stdout.write(f\"\\r‚è≥  Status: {status:<12} (Refreshing in {remaining}s)\")\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(1)\n",
    "\n",
    "# Clear the last status line and show final output\n",
    "sys.stdout.write('\\033[K')  # Clear line\n",
    "print(f\"\\nüèÅ  Job finished ‚Üí {job['status']}\")\n",
    "output_model = job[\"output_model\"]\n",
    "print(output_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc9bffd-a33d-48fb-9017-633da1761c53",
   "metadata": {},
   "source": [
    "Let's check the metrics from our customization job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09fae26-7623-459f-bcdf-d51c23da5767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_training_metrics(job_data):\n",
    "    metrics = job_data['status_details']['metrics']['metrics']\n",
    "    \n",
    "    # Extract training loss data\n",
    "    train_loss = pd.DataFrame(metrics['train_loss'])\n",
    "    train_loss['timestamp'] = pd.to_datetime(train_loss['timestamp'])\n",
    "    \n",
    "    # Extract validation loss data\n",
    "    val_loss = pd.DataFrame(metrics['val_loss'])\n",
    "    val_loss['timestamp'] = pd.to_datetime(val_loss['timestamp'])\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"Training Summary:\")\n",
    "    print(f\"Final Training Loss: {train_loss['value'].iloc[-1]:.4f}\")\n",
    "    print(f\"Best Training Loss: {train_loss['value'].min():.4f}\")\n",
    "    print(f\"Best Validation Loss: {val_loss['value'].min():.4f}\")\n",
    "    print(f\"Number of Training Steps: {train_loss['step'].max()}\")\n",
    "    print(f\"Number of Epochs: {job_data['status_details']['epochs_completed']}\")\n",
    "    print(f\"Best Epoch: {job_data['status_details']['best_epoch']}\")\n",
    "    \n",
    "    # Create a plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_loss['step'], train_loss['value'], label='Training Loss')\n",
    "    plt.scatter(val_loss['step'], val_loss['value'], color='red', label='Validation Loss')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "display_training_metrics(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3945ab76-bfe7-48a6-bd8d-9ecdbb733a7d",
   "metadata": {},
   "source": [
    "The `output_model` field returned above contains the HF-style path of the\n",
    "fine-tuned adapter (e.g.  \n",
    "`default/meta-llama-3.1-8b-instruct-legal_dataset_notebook_2-lora@cust-xyz‚Ä¶`).\n",
    "\n",
    "You can now:\n",
    "\n",
    "1. Start a new NIM container with this adapter mounted, **or**  \n",
    "2. Load the adapter into the existing NIM endpoint via the runtime API (for supported runtimes) - this option is already available for you to use\n",
    "\n",
    "Refer back to Notebook 2 for quick health checks & inference examples. We will use the same prompt to have a basic sanity check of the trained Lora adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ad64b0-ea94-4a2e-bfb8-bf0e23457edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Generate a concise, engaging title for the following legal question on an internet forum. The title should be legally relevant, capture key aspects of the issue, and entice readers to learn more. QUESTION: The Small Claims Court in California say that they cannot collect the value awarded by any sentence that they deliver. What happens to the defendant if he/she is sentenced to pay the plaintiff but refuses to pay the claim? Is the defendant liable of further penalties? Should the case proceed to a higher court? The Small Claims website does says about \"more serious steps\", but it is not clear what these are. \\nTITLE: '\n",
    "pp(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b4938-bd1c-4dcc-b23d-42c73193f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    \"model\": output_model,\n",
    "    \"prompt\": \"When responding, provide only one clear, final answer option without listing alternatives, and avoid using phrases like 'option 1' or 'option 2'.\\n\" + prompt,\n",
    "    \"temperature\": 0.2,\n",
    "    \"nvext\": {\"top_k\": 1, \"top_p\": 0.0},\n",
    "    \"max_tokens\": 75,\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{nim_url}/v1/completions\", json=request_body)\n",
    "\n",
    "print(resp.json()[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8eb62c-491e-4020-81eb-b88b6c9e39b3",
   "metadata": {},
   "source": [
    "We can now compare it to the behavior of the base model - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9bc4bc-eae5-4262-a8b4-4b0bd1672138",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    \"model\": NIM_model_id,\n",
    "    \"prompt\": \"When responding, provide only one clear, final answer option without listing alternatives, and avoid using phrases like 'option 1' or 'option 2'.\\n\" + prompt,\n",
    "    \"temperature\": 0.2,\n",
    "    \"nvext\": {\"top_k\": 1, \"top_p\": 0.0},\n",
    "    \"max_tokens\": 75,\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{nim_url}/v1/completions\", json=request_body)\n",
    "\n",
    "print(resp.json()[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836efc9-1a35-4900-94c1-6139c2df3da0",
   "metadata": {},
   "source": [
    "Based on the two outputs above, what's your general feeling about the quality of the model after fine-tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ba8394-e8a0-409f-b50f-fb340afa770c",
   "metadata": {},
   "source": [
    "# Assessment: Evaluating Your Fine-tuned Model\n",
    "\n",
    "## Overview\n",
    "\n",
    "You've now successfully fine-tuned a model using LoRA adaptation. In this assessment, you'll conduct a comprehensive evaluation to understand the impact of your fine-tuning. This evaluation is crucial for:\n",
    "\n",
    "- Verifying improvements in legal domain expertise\n",
    "- Ensuring preservation of general capabilities\n",
    "- Making data-driven recommendations for future applications\n",
    "\n",
    "## Part 1: Domain-Specific Evaluation\n",
    "\n",
    "### Task 1.1: Legal Domain Performance\n",
    "Use the NeMo Evaluator to compare your LoRA-adapted model against the baseline:\n",
    "\n",
    "```python\n",
    "# Configure similarity metrics evaluation\n",
    "legal_eval_config = {\n",
    "    \"type\": \"similarity_metrics\",\n",
    "    \"name\": \"legal_domain_assessment\",\n",
    "    \"tasks\": {\n",
    "        \"legal_summary\": {\n",
    "            \"dataset\": {\"files_url\": f\"hf://datasets/default/{dataset_name}/testing/testing.jsonl\"},\n",
    "            \"metrics\": {\n",
    "                \"bleu\": {\"type\": \"bleu\"},\n",
    "                \"f1\": {\"type\": \"f1\"},\n",
    "                \"rouge\": {\"type\": \"rouge\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Task 1.2: LLM-as-Judge Assessment\n",
    "Have a larger LLM evaluate the quality of responses:\n",
    "\n",
    "```python\n",
    "# Configure LLM-as-Judge evaluation\n",
    "judge_eval_config = {\n",
    "    \"type\": \"llm_as_judge\",\n",
    "    \"name\": \"legal_quality_assessment\",\n",
    "    \"tasks\": {\n",
    "        \"legal_evaluation\": {\n",
    "            \"model\": {\n",
    "                \"api_endpoint\": {\n",
    "                    \"url\": ...,\n",
    "                    \"model_id\": \"nvdev/nvidia/llama-3.3-nemotron-super-49b-v1\",\n",
    "                    \"api_key\": ...\n",
    "                }\n",
    "            },\n",
    "            \"template\": {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a legal expert evaluating the quality of responses to legal questions.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": \"Question: {{ item.question }}\\nResponse: {{ sample.output_text }}\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## Part 2: General Knowledge Verification\n",
    "\n",
    "### Task 2.1: Mathematical Reasoning\n",
    "Think of a good metric for knoweledge verification and use it to verify the current capabilities:\n",
    "\n",
    "```python\n",
    "# Configure GSM8K evaluation\n",
    "math_eval_config = {\n",
    "    \"type\": \"...\",\n",
    "    \"name\": \"math_assessment\",\n",
    "    \"tasks\": {\n",
    "        \"...\": {\n",
    "            \"type\": \"...\",\n",
    "            \"params\": {\n",
    "                \"limit_samples\": 50,\n",
    "                \"temperature\": 0.0001,\n",
    "                \"max_tokens\": 256\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Task 2.2: General QA Performance\n",
    "Evaluate on general knowledge questions:\n",
    "\n",
    "```python\n",
    "# Configure general QA evaluation\n",
    "qa_eval_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"name\": \"general_qa_assessment\",\n",
    "    \"tasks\": {\n",
    "        \"qa_evaluation\": {\n",
    "            \"dataset\": {\"files_url\": \"hf://datasets/default/general_qa/test.jsonl\"},\n",
    "            \"metrics\": {\n",
    "                \"accuracy\": {\"type\": \"exact_match\"},\n",
    "                \"f1\": {\"type\": \"f1\"},\n",
    "                \"completeness\": {\n",
    "                    \"type\": \"llm-judge\",\n",
    "                    \"params\": {\n",
    "                        \"model\": {\n",
    "                            \"api_endpoint\": {\n",
    "                                \"url\": ...,\n",
    "                                \"model_id\": \"nvdev/nvidia/llama-3.3-nemotron-super-49b-v1\",\n",
    "                                \"api_key\": ...\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## Part 3: Analysis Questions\n",
    "\n",
    "After running these evaluations, analyze your results to answer the following questions:\n",
    "\n",
    "### 3.1 Domain Expertise \n",
    "- [ ] How much did BLEU and F1 scores improve on legal tasks? \n",
    "- [ ] Which specific aspects of legal reasoning improved the most? \n",
    "- [ ] Compare LoRA performance vs. ICL on complex legal questions \n",
    "\n",
    "### 3.2 General Capabilities \n",
    "- [ ] Quantify any changes in GSM8K performance \n",
    "- [ ] Analyze response quality on general knowledge QA \n",
    "- [ ] Identify any areas of capability degradation \n",
    "\n",
    "### 3.3 Trade-off Analysis\n",
    "- [ ] Compare computational costs of LoRA vs. ICL \n",
    "- [ ] Analyze latency and resource requirements \n",
    "- [ ] Provide specific use case recommendations \n",
    "\n",
    "## Part 4: Recommendations\n",
    "\n",
    "Complete this template with your findings:\n",
    "\n",
    "```python\n",
    "recommendations = {\n",
    "    \"Use LoRA when\": [\n",
    "        \"1. ...\",\n",
    "        \"2. ...\",\n",
    "        \"3. ...\"\n",
    "    ],\n",
    "    \"Use ICL when\": [\n",
    "        \"1. ...\",\n",
    "        \"2. ...\",\n",
    "        \"3. ...\"\n",
    "    ],\n",
    "    \"Potential Improvements\": [\n",
    "        \"1. ...\",\n",
    "        \"2. ...\",\n",
    "        \"3. ...\"\n",
    "    ]\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
