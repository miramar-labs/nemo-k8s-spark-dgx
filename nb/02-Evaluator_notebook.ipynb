{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708e6471-4bad-4ea3-b757-4f63853f84fb",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8438c60-460d-4fd6-852f-ae43e82ea8fc",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\">**Notebook 2:** Evaluating LLM Performance on Legal Domain Tasks Using NeMo Evaluator\n",
    "</font>\n",
    "\n",
    "### Overview\n",
    "\n",
    "Large Language Models (LLMs) have revolutionized AI by learning from vast amounts of general-purpose text data. While this makes them versatile across many domains, specific applications like legal analysis may require evaluating and potentially enhancing their domain expertise.\n",
    "\n",
    "This notebook demonstrates how to rigorously evaluate an LLM's performance on legal domain tasks using the NeMo Evaluator framework. We'll explore three key evaluation approaches:\n",
    "\n",
    "1. **Zero-Shot Performance**: Testing the model's base capabilities without any context\n",
    "2. **In-Context Learning (ICL)**: Enhancing performance by providing relevant examples in the prompt\n",
    "3. **LLM-as-a-Judge**: Using a larger LLM to assess output quality\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "In this notebook you will learn how to:\n",
    "- Set up and configure NeMo Microservices for evaluation\n",
    "- Prepare custom legal datasets and upload them to NeMo Data Store\n",
    "- Evaluate LLM performance using similarity metrics (ROUGE) and LLM-as-a-Judge\n",
    "- Compare model performance with and without in-context learning\n",
    "- Visualize and analyze evaluation results using MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d7e5d-374a-4b60-a859-b5334f47593d",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "## **Getting Started With Evaluating NIM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf77105a-5369-4eb9-adb5-d53cdf4d95a3",
   "metadata": {},
   "source": [
    "### What is In-Context Learning (ICL)?\n",
    "\n",
    "In-context learning (ICL) is a technique where we provide the model with a few carefully chosen examples alongside the main question. This notebook compares two modes:\n",
    "\n",
    "- **Zero-Shot**: Model receives only the question\n",
    "  ```\n",
    "  Question: What are the legal implications of remote work across state lines?\n",
    "  ```\n",
    "\n",
    "- **Few-Shot (ICL)**: Model receives examples plus the question\n",
    "  ```\n",
    "  Example 1:\n",
    "  Question: What constitutes workplace discrimination?\n",
    "  Answer: Workplace discrimination occurs when an employer treats an individual differently based on protected characteristics like race, gender, age, religion, or disability. This includes hiring, firing, promotion, and workplace conditions.\n",
    "\n",
    "  Example 2:\n",
    "  Question: How does copyright law apply to software?\n",
    "  Answer: Software copyright protects original code, giving creators exclusive rights to reproduce, modify, distribute, and license their work. Both source code and object code are protected under copyright law.\n",
    "\n",
    "  Question: What are the legal implications of remote work across state lines?\n",
    "  ```\n",
    "\n",
    "This comparison will help quantify how much ICL improves performance on legal domain tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76768faf",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "2. [Install Python Package Requirements](#Install-Python-Package-Requirements)\n",
    "3. [Set up MLflow for Experiment Tracking](#Set-up-MLflow-for-Experiment-Tracking)\n",
    "4. [Configure NeMo Microservices Endpoints](#Set-up-NeMo-Microservice-API-endpoints)\n",
    "5. [Download & Format Dataset](#Download-and-Format-a-Dataset)\n",
    "6. [Inference with Locally Deployed NIM](#Inference-with-Locally-Deployed-NIM)\n",
    "7. [Evaluation with NeMo Evaluator MS](#Evaluate-on-Legal-dataset-using-Nemo-Evaluator-MS)\n",
    "   * [Zero-Shot Mode](#Evaluation-of-zero-shot-mode)\n",
    "   * [Few-Shot (ICL) Mode](#Evaluation-of-few-shots-(ICL))\n",
    "8. [LLM-as-a-Judge Evaluation](#Evaluations-with-LLM-as-a-Judge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64dd58",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc4a14-37ab-40a4-80bf-b4c2a756f2c7",
   "metadata": {},
   "source": [
    "To run an evaluation, the following NeMo Microservices are required:\n",
    "* NeMo Data Store for managing datasets used to train a custom model and for storing the trained model adapter\n",
    "* Nemo Entity Store for managing platform-wide entities\n",
    "* Access to NVIDIA NIM with Llama 3.2-3B-Instruct model. It comes in the form a docker container that can be obtained from the [NGC Catalog](https://catalog.ngc.nvidia.com/orgs/nim/teams/meta/containers/llama-3.2-3b-instruct).\n",
    "  * (Optional) You can modify the notebook to train with another supported model by updating `config_model_id`\n",
    "* As we also want to use a judge LLM - Llama-3.3-70B-instruct, you will also need to have an NGC_API_KEY to use the [build.nvidia.com](https://build.nvidia.com/meta/llama-3_3-70b-instruct) platform \n",
    "* NeMo Evaluator Microservice\n",
    "\n",
    "We have already deployed the microservices in this environment, but if you want to dive into how to deploy it, you can see the documentation for [Getting Started](https://developer.nvidia.com/docs/nemo-microservices/parent-chart/source/getting-started.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb8834b-e8e6-4ab2-ab98-43ff1f360c7f",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/architecture-topology.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35eee4a-d420-47c0-b9a6-f6d5bb8fd669",
   "metadata": {},
   "source": [
    "The Nemo Evaluator MS supports a wide variety of evaluation tasks and metrics. We won't cover all of them, but we will give you the tools to go through the process of deploying your first evaluation job, and how you can customize it in your future evaluation jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb7188",
   "metadata": {},
   "source": [
    "### Install Python Package Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c135a",
   "metadata": {},
   "source": [
    "Nemo Evaluator Microservice is supporting [Hugging Face API](https://huggingface.co/docs/huggingface_hub/package_reference/hf_api) to version control the datasets and models used for evaluation.\n",
    "<div></div>\n",
    "We will also install the datatsets repository, so we can download our dataset and format it for our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d7c04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (2.32.5)\n",
      "Requirement already satisfied: datasets in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (4.5.0)\n",
      "Requirement already satisfied: bs4 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (0.0.2)\n",
      "Requirement already satisfied: ftfy in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (6.3.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: filelock in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from datasets) (3.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from datasets) (2.4.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from datasets) (1.3.7)\n",
      "Requirement already satisfied: packaging in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.21.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from bs4) (4.14.3)\n",
      "Requirement already satisfied: wcwidth in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from ftfy) (0.2.14)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (1.3.7)\n",
      "Requirement already satisfied: filelock in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface_hub) (3.20.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface_hub) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface_hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface_hub) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface_hub) (0.21.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: anyio in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (4.12.0)\n",
      "Requirement already satisfied: certifi in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub) (0.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/aaron/.pyenv/versions/pyNeMo/lib/python3.11/site-packages (from typer-slim->huggingface_hub) (8.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install requests datasets bs4 ftfy\n",
    "! pip install -U \"huggingface_hub\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705af7fd-8e4c-409c-93fa-6b61e4e900e1",
   "metadata": {},
   "source": [
    "To visualize our evaluation results, we will use the open-source library [mlflow](https://mlflow.org/); We will use mlflow-tracking and the integration with Nemo Evaluator MS to ingest the evaluation results and visualize them. \n",
    "<div></div>\n",
    "MLflow Tracking is a component of the MLflow platform that enables data scientists to record and manage the parameters, metrics, and artifacts of machine learning experiments. With MLflow Tracking, users can log experiment metadata, such as hyperparameters, metrics, and model artifacts, to a centralized tracking server. This allows for easy comparison and reproduction of experiments, as well as the ability to visualize and analyze results using tools like plots, tables, and charts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acf9f94-37c4-4e6e-8ccf-f9e97d6b8aaa",
   "metadata": {},
   "source": [
    "### Set up MLflow for Experiment Tracking\n",
    "\n",
    "[MLflow](https://mlflow.org/) is an open-source platform that helps manage the machine learning lifecycle, including experimentation, reproducibility, and deployment. We'll use it to:\n",
    "- Track evaluation parameters and metrics\n",
    "- Compare results across different evaluation approaches\n",
    "- Visualize performance differences between zero-shot and few-shot modes\n",
    "\n",
    "#### 1. Verify MLflow Server Connection\n",
    "\n",
    "The MLflow tracking server is already configured and accessible. Let's verify the connection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3381fd0",
   "metadata": {},
   "source": [
    "You will need to create a NodePort for MLflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b48cbcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/mlflow-tracking patched (no change)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "kubectl -n mlflow-system patch svc mlflow-tracking --type='merge' -p '{\n",
    "  \"spec\": {\n",
    "    \"type\": \"NodePort\",\n",
    "    \"ports\": [\n",
    "      { \"name\": \"http\", \"port\": 80, \"targetPort\": \"mlflow\", \"protocol\": \"TCP\", \"nodePort\": 30090 }\n",
    "    ]\n",
    "  }\n",
    "}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89ad745-280d-4342-ba96-daa71bac3408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MLflow server is accessible on port 30090\n"
     ]
    }
   ],
   "source": [
    "# MLflow is already exposed via NodePort + socat (configured during environment setup).\n",
    "# Verify the connection is working:\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    resp = requests.get(\"http://192.168.49.2:30090/\", timeout=5)\n",
    "    if resp.status_code == 200:\n",
    "        print(\"✓ MLflow server is accessible on port 30090\")\n",
    "    else:\n",
    "        print(f\"⚠ MLflow returned status code: {resp.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"✗ Could not connect to MLflow: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d396ca-42e2-4d4e-b601-c1fce56b8fe4",
   "metadata": {},
   "source": [
    "#### 2. Access MLflow UI\n",
    "\n",
    "Run this cell to generate a clickable link to the MLflow interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4ac27-7741-4da9-891f-cbf17ce84de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "var url = 'http://'127.0.0.1':30090';\nelement.innerHTML = '<a style=\"color:#76b900;\" target=\"_blank\" href='+url+'><h2>< Link To MLFLOW UI ></h2></a>';\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = 'http://'+window.location.host+':30090';\n",
    "element.innerHTML = '<a style=\"color:#76b900;\" target=\"_blank\" href='+url+'><h2>< Link To MLFLOW UI ></h2></a>';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ef5d2-3d84-4647-8b50-0a3cecd4815f",
   "metadata": {},
   "source": [
    "#### 3. Set MLflow URI\n",
    "\n",
    "Define the MLflow tracking URI for logging our evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6caf89ad-dad5-451a-afa5-bf17140e5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_uri = \"http://192.168.49.2:30090\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8cb2e-c86f-4103-aba6-bebcf517c8fb",
   "metadata": {},
   "source": [
    "#### What We'll Track\n",
    "\n",
    "Our MLflow experiments will capture:\n",
    "- Model configurations (zero-shot vs few-shot)\n",
    "- Evaluation metrics (ROUGE, BLEU, F1)\n",
    "- LLM-as-Judge scores\n",
    "- Performance comparisons across different approaches\n",
    "\n",
    "This structured tracking will help us:\n",
    "1. Compare the effectiveness of in-context learning\n",
    "2. Analyze which evaluation metrics best capture model improvements\n",
    "3. Visualize performance patterns across different legal queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d664fff",
   "metadata": {},
   "source": [
    "### Set up NeMo Microservice API endpoints\n",
    "\n",
    "<a id=\"init-vars\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37c993-fba3-4301-bbab-d6d500099b05",
   "metadata": {},
   "source": [
    "We use ingress in the server deployment so it will be easier to query the services from outside the K8s cluster; it is a design choice, not a requirement, so you can decide if you follow a similar approach when you try it. We now want to see the ingress hosts that are deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c7cf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE   NAME                            CLASS   HOSTS                                ADDRESS        PORTS   AGE\n",
      "default     nemo-microservices-helm-chart   nginx   nemo.test,nim.test,data-store.test   192.168.49.2   80      18d\n"
     ]
    }
   ],
   "source": [
    "!kubectl get ingress --all-namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088ca3c6-c765-4696-9c8e-b57c0474d554",
   "metadata": {},
   "source": [
    "For the evaluation, we will evaluate Meta Llama-3.1-8B-instruct-dgx-spark that we deployed locally, you can use other SLMs by replacing the deployed NIM with a different [NIMs](https://docs.nvidia.com/nim/large-language-models/latest/supported-models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c074f1-c1a2-44df-adf9-8cc87cfa32f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":[{\"created\":1770173621,\"id\":\"meta/llama-3.1-8b-instruct-dgx-spark\",\"object\":\"model\",\"owned_by\":\"vllm\",\"permission\":[{\"created\":1770173621,\"id\":\"modelperm-4fe075cd98f84f8fb16813437e294454\",\"object\":\"model_permission\",\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}],\"root\":\"/opt/nim/workspace\",\"parent\":\"\"}]}\n"
     ]
    }
   ],
   "source": [
    "!curl http://nim.test/v1/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5267a4-e1ac-426f-91fe-157c7d088ffc",
   "metadata": {},
   "source": [
    "Set the following variables to the hostname of each microservice before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5171567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minikube ip: 192.168.49.2\n",
    "datastore_url = \"http://data-store.test\"\n",
    "nim_url = \"http://nim.test\"\n",
    "eval_url = \"http://nemo.test\"\n",
    "customizer_url = \"http://nemo.test\"\n",
    "entitystore_url = \"http://nemo.test\"\n",
    "\n",
    "NIM_model_id = \"meta/llama-3.1-8b-instruct-dgx-spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670c1996-ebab-4afa-b8b9-4050aa02ce49",
   "metadata": {},
   "source": [
    "We will also use hosted end-points for our evaluation. For this workshop environment, to avoid you needing to provide an API key, we will be sending requests through a proxy service we already have running in a background that will inject a valid API key. We'll mock an API key just so we can supply it to functions that expect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "130e8287-599b-487f-a362-7d2a9ade0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# We mock the API key as a valid key will be provided by a proxy service we run for this workshop environment.\n",
    "NGC_API_KEY = 'nvapi-mock-key'\n",
    "BASE_URL = 'http://proxy/v1/chat/completions'\n",
    "# Outside this workshop environment you would set the base URL as shown below.\n",
    "# BASE_URL = \"https://integrate.api.nvidia.com/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84efec8",
   "metadata": {},
   "source": [
    "We can verify the following services are running and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01da6b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nim: 200\n",
      "datastore: 200\n",
      "entitystore: 200\n",
      "evaluator: 200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pprint import pp\n",
    "\n",
    "# NIM proxy: routed\n",
    "print(\"nim:\", requests.get(f\"{nim_url}/v1/models\").status_code)\n",
    "\n",
    "# Data store: routed (since / matches everything on data-store.test)\n",
    "print(\"datastore:\", requests.get(f\"{datastore_url}/v1/health\").status_code)\n",
    "\n",
    "# Entity store: routed via catch-all host\n",
    "print(\"entitystore:\", requests.get(f\"{entitystore_url}/v1/namespaces\").status_code)\n",
    "\n",
    "# Evaluator: routed via catch-all host\n",
    "print(\"evaluator:\", requests.get(f\"{eval_url}/v1/evaluation/targets\").status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa4a7c",
   "metadata": {},
   "source": [
    "### Download and Format a Dataset\n",
    "\n",
    "The [Law StackExchange dataset](https://huggingface.co/datasets/ymoslem/Law-StackExchange) is a collection of legal questions and answers from StackExchange, up to August 2023. Each record consists of a question, some context as well as human-provided answers.\n",
    "\n",
    "We will use the Law StackExchange dataset to build an evaluation dataset to measure the NIM LLM (Llama-3.2-3B-instruct) with and without In-context learning in generating engaging titles for legal questions. Download the dataset from HuggingFace and preprocess the dataset using [NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator) to cleanse HTML tags, reformat unicode, and filter samples by word count and score thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf4e578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from time import sleep\n",
    "\n",
    "import ftfy\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1027c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question_title', 'score', 'question_body'],\n",
      "    num_rows: 24370\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hf_ds = load_dataset(\"ymoslem/Law-StackExchange\")\n",
    "hf_ds = hf_ds[\"train\"].remove_columns(\n",
    "    [\"link\", \"license\", \"question_id\", \"answers\", \"tags\"]\n",
    ")\n",
    "print(hf_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02472f1d",
   "metadata": {},
   "source": [
    "Filter rows based on score and content length to curate a quality dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86854163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question_title', 'score', 'question_body'],\n",
      "    num_rows: 916\n",
      "})\n",
      "{'question_title': 'Why is drunk driving causing accident punished so much '\n",
      "                   'worse than just drunk driving?',\n",
      " 'score': 23,\n",
      " 'question_body': '<p>When people drink and drive and then cause an accident '\n",
      "                  'especially where if someone dies they get years and years '\n",
      "                  'in prison but just the act of drunk driving is punished way '\n",
      "                  \"more lenient.  Shouldn't the 2, drunk driving and drunk \"\n",
      "                  'driving then causing accident be similarly punished?  I '\n",
      "                  \"feel like a lot of times it's luck whether an accident \"\n",
      "                  'happens.</p>\\n'}\n"
     ]
    }
   ],
   "source": [
    "filtered_ds = hf_ds.filter(lambda row: row[\"score\"] > 10)\n",
    "filtered_ds = filtered_ds.filter(\n",
    "    lambda row: len(row[\"question_body\"]) >= 50 and len(row[\"question_body\"]) <= 1000\n",
    ")\n",
    "print(filtered_ds)\n",
    "pp(filtered_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662bc72",
   "metadata": {},
   "source": [
    "Modify the dataset and clean text from HTML tags and reformat unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e956d15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question_title', 'score', 'question_body'],\n",
      "    num_rows: 916\n",
      "})\n",
      "{'question_title': 'Why is drunk driving causing accident punished so much '\n",
      "                   'worse than just drunk driving?',\n",
      " 'score': 23,\n",
      " 'question_body': 'When people drink and drive and then cause an accident '\n",
      "                  'especially where if someone dies they get years and years '\n",
      "                  'in prison but just the act of drunk driving is punished way '\n",
      "                  \"more lenient. Shouldn't the 2, drunk driving and drunk \"\n",
      "                  'driving then causing accident be similarly punished? I feel '\n",
      "                  \"like a lot of times it's luck whether an accident happens.\"}\n"
     ]
    }
   ],
   "source": [
    "modified_ds = filtered_ds.map(\n",
    "    lambda row: {\n",
    "        \"question_body\": ftfy.fix_text(\n",
    "            re.sub(\n",
    "                r\"\\s+\",\n",
    "                \" \",\n",
    "                BeautifulSoup(row[\"question_body\"], \"html.parser\").get_text(),\n",
    "            ).strip()\n",
    "        ),\n",
    "        \"question_title\": ftfy.fix_text(\n",
    "            re.sub(\n",
    "                r\"\\s+\",\n",
    "                \" \",\n",
    "                BeautifulSoup(row[\"question_title\"], \"html.parser\").get_text(),\n",
    "            ).strip()\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "print(filtered_ds)\n",
    "pp(modified_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ff3246",
   "metadata": {},
   "source": [
    "We will convert the dataset into jsonl format, with each line of the file containing an example of prompt and ideal response field.\n",
    "\n",
    "```\n",
    "{\"prompt\": \"<input>\", \"ideal_response\": \"<output>\"}\n",
    "```\n",
    "\n",
    "As we are focusing on a summarization evaluation, we will use `question_body` as the input for our model for summarization task and set the output to `question_title`. The next step is to rename the dataset columns from \"question_body\" and \"question_title\" to the required \"prompt\" and \"ideal response\" format for NeMo Evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "295a061d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['completion', 'prompt'],\n",
      "    num_rows: 916\n",
      "})\n",
      "{'completion': 'Why is drunk driving causing accident punished so much worse '\n",
      "               'than just drunk driving?',\n",
      " 'prompt': 'When people drink and drive and then cause an accident especially '\n",
      "           'where if someone dies they get years and years in prison but just '\n",
      "           \"the act of drunk driving is punished way more lenient. Shouldn't \"\n",
      "           'the 2, drunk driving and drunk driving then causing accident be '\n",
      "           \"similarly punished? I feel like a lot of times it's luck whether \"\n",
      "           'an accident happens.'}\n"
     ]
    }
   ],
   "source": [
    "nemo_ds = (\n",
    "    modified_ds.rename_column(\"question_body\", \"prompt\")\n",
    "    .rename_column(\"question_title\", \"completion\")\n",
    "    .remove_columns([\"score\"])\n",
    ")\n",
    "print(nemo_ds)\n",
    "pp(nemo_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10498a9",
   "metadata": {},
   "source": [
    "#### Split the Dataset\n",
    "\n",
    "Split the dataset to training, validation, and evaluation. We will use the training and validation dataset in the next notebook with the Nemo Customizer MS. In this notebook, we will focus on the evaluation dataset. We picked a rather small evaluation set size (46 rows) to shorten the evaluation time during the lab, but you can modify its size using the test_size parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d251713",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = nemo_ds.train_test_split(test_size=0.5, shuffle=False)\n",
    "training_dataset = ds[\"train\"]\n",
    "\n",
    "ds = ds[\"test\"].train_test_split(test_size=0.1, shuffle=False)\n",
    "validation_dataset = ds[\"train\"]\n",
    "evaluation_dataset = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd4f8cd4-3649-4f71-afca-2395404ccec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['completion', 'prompt'],\n",
      "    num_rows: 458\n",
      "})\n",
      "Dataset({\n",
      "    features: ['completion', 'prompt'],\n",
      "    num_rows: 412\n",
      "})\n",
      "Dataset({\n",
      "    features: ['completion', 'prompt'],\n",
      "    num_rows: 46\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "pp(training_dataset)\n",
    "pp(validation_dataset)\n",
    "pp(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae34dfee-e3e9-4338-b065-c61f964516b3",
   "metadata": {},
   "source": [
    "#### For In-context Learning - we will pick a set of good demonstration pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb98dbb7-b81f-4cda-94b8-304414f4b94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some examples of legal questions with good titles:\n",
      "Q: Do police officers have to stop an interrogation when right to counsel has been invoked by a suspect or can they continue the questioning like nothing has happened?\n",
      "Title: Must the interrogation stop when the right to counsel has been invoked?\n",
      "\n",
      "Q: What alternatives exist for finding and appointing an executor for one's will/estate for a person with no close family or qualified friend? Are there pros and cons?\n",
      "Title: What options are there for executor when no close family member is available?\n",
      "\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "NUM_FEW_SHOTS = 10  # adjust as you wish\n",
    "random.seed(42)\n",
    "\n",
    "# pick k examples from the train split\n",
    "few_shot_examples = random.sample(list(training_dataset), NUM_FEW_SHOTS)\n",
    "\n",
    "\n",
    "def build_few_shot_context(examples):\n",
    "    \"\"\"\n",
    "    Build a text block like:\n",
    "        Here are examples...\n",
    "        Question: <body>\n",
    "        Title: <title>\n",
    "        ...\n",
    "    \"\"\"\n",
    "    lines = [\"Here are some examples of legal questions with good titles:\"]\n",
    "    for ex in examples:\n",
    "        lines.append(f\"Q: {ex['prompt']}\")\n",
    "        lines.append(f\"Title: {ex['completion']}\")\n",
    "        lines.append(\"\")  # blank line between examples\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "FEW_SHOT_CONTEXT = build_few_shot_context(few_shot_examples)\n",
    "\n",
    "FEW_SHOT_CONTEXT_SHORT = build_few_shot_context(few_shot_examples[:2])\n",
    "print(FEW_SHOT_CONTEXT_SHORT)\n",
    "print(\"-----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7a0a8",
   "metadata": {},
   "source": [
    "Now, save the preprocessed training and validation datasets to local files for uploading to NeMo Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4529c203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf6e60e12554a4ab83fad9914daec97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37526034332b4da591cc46f91469f4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion': 'Which jurisdiction applies to copyright violations on the '\n",
      "               'internet?',\n",
      " 'prompt': 'A person residing in country A takes a work by an artist in '\n",
      "           'country B and puts it onto a website they own but which is hosted '\n",
      "           'in country C which is intended for an audience of people in '\n",
      "           'country D. The artist in country B did not give permission for '\n",
      "           'this and wants to pursue legal actions. Which countries copyright '\n",
      "           \"laws apply to this case? Let's assume that A, B, C and D all \"\n",
      "           'signed and ratified the Berne Convention, but their '\n",
      "           'implementations in local laws differ in ways which are relevant to '\n",
      "           'this case.'}\n"
     ]
    }
   ],
   "source": [
    "training_file = \"law-qa-train.jsonl\"\n",
    "validation_file = \"law-qa-val.jsonl\"\n",
    "testing_file = \"law-qa-test.jsonl\"\n",
    "\n",
    "training_dataset.to_json(\n",
    "    training_file\n",
    ")  # saves dataset to the specified \"training_file\" file path\n",
    "validation_dataset.to_json(\n",
    "    validation_file\n",
    ")  # saves dataset to the specified \"validation_file\" file path\n",
    "\n",
    "pp(evaluation_dataset[1])  # print a sample of the dataset to check validity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020660fe",
   "metadata": {},
   "source": [
    "We will add a category column to align with the expected format for the Evaluator MS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aa264fa-acb5-45b4-8cce-9b8992ee00d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1bd343ad454011a609ff13bb62ba77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'completion': 'Do I lose my rights as a British citizen when I travel to an '\n",
      "               'other country for tourism?',\n",
      " 'prompt': 'A friend of mine got detained at the airport in Jordan because his '\n",
      "           'name matches a name of someone who has issues with the Jordanian '\n",
      "           'authorities. My friend is British and he only was passing through '\n",
      "           'Jordan. They forced him to stay there for 24 hours with no food '\n",
      "           'and he had to sleep on the floor before they determined that he is '\n",
      "           'not the man they were after. Does this incident mean that when you '\n",
      "           'travel to a foreign country – even for a short time – that you '\n",
      "           'give up your rights as a British citizen?',\n",
      " 'category': 'summarization'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb6955a71f547bbbd6aff727868a0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "25156"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_dataset = evaluation_dataset.map(  # we are removing the score column as it's not providing additional value for the custom evaluation task\n",
    "    lambda line: {\"category\": \"summarization\"}\n",
    ")\n",
    "pp(inputs_dataset[2])\n",
    "inputs_dataset.to_json(\n",
    "    testing_file\n",
    ")  # saves dataset to the specified \"testing_file\" file path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c7b4e3",
   "metadata": {},
   "source": [
    "#### Create a Dataset in NeMo Data Store\n",
    "\n",
    "Now that the files have been preprocessed, add the dataset to NeMo Data Store. First create a dataset then upload the files to NeMo Data Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb7d135",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "Client error '409 Conflict' for url 'http://data-store.test/v1/hf/api/repos/create'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/409\n\nYou already created this repo",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/pyNeMo/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:657\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/pyNeMo/lib/python3.11/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '409 Conflict' for url 'http://data-store.test/v1/hf/api/repos/create'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/409",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m hf_api = HfApi(datastore_url + \u001b[33m\"\u001b[39m\u001b[33m/v1/hf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mhf_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     14\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSince you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve run the cell before, the repo has already been created in the NeMo Data Store\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/pyNeMo/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:89\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     85\u001b[39m         validate_repo_id(arg_value)\n\u001b[32m     87\u001b[39m kwargs = smoothly_deprecate_legacy_arguments(fn_name=fn.\u001b[34m__name__\u001b[39m, kwargs=kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/pyNeMo/lib/python3.11/site-packages/huggingface_hub/hf_api.py:3940\u001b[39m, in \u001b[36mHfApi.create_repo\u001b[39m\u001b[34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[39m\n\u001b[32m   3937\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3939\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3940\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3941\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HfHubHTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   3942\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err.response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m   3943\u001b[39m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/pyNeMo/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:752\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    750\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    751\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: Client error '409 Conflict' for url 'http://data-store.test/v1/hf/api/repos/create'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/409\n\nYou already created this repo"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "dataset_name = \"legal_dataset_notebook_2\"\n",
    "repo_id = \"default/\" + dataset_name\n",
    "repo_type = \"dataset\"\n",
    "\n",
    "hf_api = HfApi(datastore_url + \"/v1/hf\")\n",
    "\n",
    "try:\n",
    "    hf_api.create_repo(repo_id, repo_type=repo_type)\n",
    "except HTTPError:\n",
    "    print(\n",
    "        f\"Since you've run the cell before, the repo has already been created in the NeMo Data Store\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dbe4bf",
   "metadata": {},
   "source": [
    "#### Upload Dataset Files\n",
    "\n",
    "We will now upload the evaluation dataset (testing) to the datastore; we won't use the training and validation, however, you can un-comment it if you'd want to use it to fine tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "231d5b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259ffe56c7bf4bc7a08edeace7355a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 0 LFS files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9774731de7e4b6cbae47eaf615d045d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 0 LFS files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c6fd091fcc4c07a162f6bd818e8239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 0 LFS files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='', commit_message='Upload testing/testing.jsonl with huggingface_hub', commit_description='', oid='8bd1bbc222c78484328b298a29ec833e69d48dbb', pr_url=None, repo_url=RepoUrl('', endpoint='http://data-store.test/v1/hf', repo_type='model', repo_id=''), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_api.upload_file(\n",
    "    path_or_fileobj=training_file,\n",
    "    path_in_repo=\"training/training.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n",
    "hf_api.upload_file(\n",
    "    path_or_fileobj=validation_file,\n",
    "    path_in_repo=\"validation/validation.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n",
    "hf_api.upload_file(\n",
    "    path_or_fileobj=testing_file,\n",
    "    path_in_repo=\"testing/testing.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf403196-50a2-48b9-8dd0-da4bc88c3944",
   "metadata": {},
   "source": [
    "#### Few-Shot (ICL) custom dataset\n",
    "  \n",
    "To measure the true impact of *in-context learning*, we want to create a dataset that will be used to compare it against the zero-shot:\n",
    "\n",
    "1. Prepend the demonstration block stored in `FEW_SHOT_CONTEXT` to every prompt in the test set.  \n",
    "2. Save this “ICL” version of the file and upload it to NeMo Data Store.  \n",
    "3. Launch a second evaluation job that points to the new file, so the Evaluator MS calls the **same NIM** but with few-shot prompts.\n",
    "\n",
    "By comparing the metrics of this run to the earlier zero-shot run we obtain a quantitative view of how much the demonstration examples help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c9faa45-81d6-49db-b56b-a29f1893fd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7818fcd0ac3540b0a55a044f54a039ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dba79c11dc0474da5ef0feaac904981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80e60258d76483183fb661a1eb66826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 0 LFS files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='', commit_message='Upload testing/testing_icl.jsonl with huggingface_hub', commit_description='', oid='fbf719f9119a680e69a469c8122e4cf87bcb7943', pr_url=None, repo_url=RepoUrl('', endpoint='http://data-store.test/v1/hf', repo_type='model', repo_id=''), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def with_few_shot(ex):\n",
    "    return {\n",
    "        # prepend the demonstration block + one blank line\n",
    "        \"prompt\": f\"{FEW_SHOT_CONTEXT}\\nQ: {ex['prompt']}\",\n",
    "        \"ideal_response\": ex[\"completion\"],\n",
    "        \"category\": ex[\"category\"],\n",
    "    }\n",
    "\n",
    "\n",
    "icl_dataset = inputs_dataset.map(with_few_shot)\n",
    "icl_testing_file = \"law-qa-test-icl.jsonl\"\n",
    "icl_dataset.to_json(icl_testing_file)\n",
    "\n",
    "# Upload to NeMo Data Store\n",
    "hf_api.upload_file(\n",
    "    path_or_fileobj=icl_testing_file,\n",
    "    path_in_repo=\"testing/testing_icl.jsonl\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1a1d5",
   "metadata": {},
   "source": [
    "Use the NeMo Data Store API to view the new dataset and verify the files have uploaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d0307a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DatasetInfo(id='default/legal_dataset_notebook_2',\n",
      "             author=None,\n",
      "             sha=None,\n",
      "             created_at=datetime.datetime(2026, 2, 3, 0, 27, 33, tzinfo=datetime.timezone.utc),\n",
      "             last_modified=datetime.datetime(2026, 2, 4, 3, 0, 5, tzinfo=datetime.timezone.utc),\n",
      "             private=None,\n",
      "             gated=None,\n",
      "             disabled=None,\n",
      "             downloads=None,\n",
      "             downloads_all_time=None,\n",
      "             likes=None,\n",
      "             paperswithcode_id=None,\n",
      "             tags=None,\n",
      "             trending_score=None,\n",
      "             card_data=None,\n",
      "             siblings=None)]\n"
     ]
    }
   ],
   "source": [
    "datasets = hf_api.list_datasets()\n",
    "pp([x for x in datasets if x.id == repo_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea4daab-eca0-4964-b96d-0003976084e0",
   "metadata": {},
   "source": [
    "#### Register the Dataset\n",
    "\n",
    "NeMo Entity Store is a microservice for managing platform-wide entities such as namespaces, models, and datasets within the NeMo microservices platform. To register the dataset in the entity-store, you need to define the `files_url` value and format it as `hf://datasets/{namespace}/{dataset name}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf3f5356-0815-4a1f-bbde-ec15b78e8d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detail': 'Dataset default/legal_dataset_notebook_2 already exists.'}\n"
     ]
    }
   ],
   "source": [
    "dataset_params = {\n",
    "    \"name\": dataset_name,\n",
    "    \"namespace\": \"default\",\n",
    "    \"description\": \"A dataset of legal issues and titles\",\n",
    "    \"files_url\": \"hf://datasets/\" + repo_id,\n",
    "    \"project\": \"my-project-id\",\n",
    "}\n",
    "\n",
    "resp = requests.post(\n",
    "    f\"{entitystore_url}/v1/datasets\", json=dataset_params, verify=False\n",
    ")\n",
    "es = resp.json()\n",
    "pp(es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec5b7e",
   "metadata": {},
   "source": [
    "### Inference with Locally Deployed NIM\n",
    "\n",
    "Before running full evaluations, let's perform some quick sanity checks with our deployed NIM model. We'll:\n",
    "1. Test basic model functionality\n",
    "2. Compare zero-shot vs few-shot responses\n",
    "3. Verify our evaluation setup\n",
    "\n",
    "#### 1. Helper Functions for Inference\n",
    "\n",
    "First, let's define some utility functions for making NIM API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a89349cb-0718-47bb-b32c-591224faf2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import textwrap\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "def make_prompt(question: str, *, with_icl: bool) -> str:\n",
    "    \"\"\"\n",
    "    Build the final prompt sent to NIM.\n",
    "    Ensures the model returns ONE line: the title only.\n",
    "    \"\"\"\n",
    "    system_rule = (\n",
    "        \"You are a headline generator for a legal Q&A forum. \"\n",
    "        \"Return ONE concise, engaging title (max 15 words) that captures \"\n",
    "        \"the core legal issue. \"\n",
    "        \"Do NOT add quotes, labels, options, or extra text. \"\n",
    "        \"Output exactly the title on a single line.\\n\"\n",
    "    )\n",
    "\n",
    "    if with_icl:\n",
    "        # Few-shot: prepend demonstration block\n",
    "        prompt = f\"{system_rule}\" f\"{FEW_SHOT_CONTEXT}\\n\" f\"Q: {question}\\n\" f\"Title:\"\n",
    "    else:\n",
    "        # Zero-shot\n",
    "        prompt = f\"{system_rule}\" f\"Q: {question}\\n\" f\"Title:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def query_nim(prompt: str, *, model_id=NIM_model_id, url=nim_url, max_tokens=75, T=0.2):\n",
    "    \"\"\"\n",
    "    Call the NIM endpoint and return the text portion only.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model_id,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": T,\n",
    "        \"nvext\": {\"top_k\": 1, \"top_p\": 0.0},\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    r = requests.post(f\"{url}/v1/completions\", json=payload, timeout=45)\n",
    "    r.raise_for_status()\n",
    "    text = r.json()[\"choices\"][0][\"text\"]\n",
    "    # Strip any accidental echo\n",
    "    return text.lstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa149e4",
   "metadata": {},
   "source": [
    "Before running the evaluations, we want to run a basic sanity check for the two scenarios. We will generate a prompt and run an inference call against the NIM model to get an initial feeling about the quality of the model with and without ICL and its responses in a domain specific knowlegde."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e949a3-d691-4fd8-b937-9751def2d105",
   "metadata": {},
   "source": [
    "#### 2. Quick Sanity Check\n",
    "\n",
    "Let's test our model with a sample legal question in both zero-shot and few-shot modes:\n",
    "\n",
    "1. **Zero-Shot** – the LLM only sees the question.  \n",
    "2. **Few-Shot / ICL** – the LLM sees the question **plus** three demonstration\n",
    "   examples (`FEW_SHOT_CONTEXT`) prepended to the prompt.\n",
    "\n",
    "Run the code cell and compare the titles generated in the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "456aa5f0-93a6-4abd-a2e5-8abb2cc05be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mGround-Truth Title\u001b[0m\n",
      "Do you have to obey English-only traffic signs in Toronto?\n",
      "\n",
      "\u001b[1mModel-Generated Titles\u001b[0m\n",
      "• Zero-Shot : Traffic Signs in English-Only Cities: Legal Obligation and Enforcement in Multilingual Canada.\n",
      "• ICL       : Are English-only traffic signs in a bilingual region still enforceable?\n"
     ]
    }
   ],
   "source": [
    "# --- Inference demo: Zero-Shot vs. ICL ----------------------------------\n",
    "demo = random.choice(evaluation_dataset)\n",
    "question_text = demo[\"prompt\"]\n",
    "ground_truth = demo[\"completion\"]\n",
    "\n",
    "# Build prompts and query the model\n",
    "zero_title = (\n",
    "    query_nim(make_prompt(question_text, with_icl=False)).splitlines()[0].strip()\n",
    ")\n",
    "icl_title = (\n",
    "    query_nim(make_prompt(question_text, with_icl=True)).splitlines()[0].strip()\n",
    ")\n",
    "\n",
    "# Display\n",
    "print(\"\\033[1mGround-Truth Title\\033[0m\")\n",
    "print(ground_truth)\n",
    "\n",
    "print(\"\\n\\033[1mModel-Generated Titles\\033[0m\")\n",
    "print(f\"• Zero-Shot : {zero_title}\")\n",
    "print(f\"• ICL       : {icl_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423c078-4b16-4391-bfc8-08f12d1095c8",
   "metadata": {},
   "source": [
    "#### 3. Analysis\n",
    "\n",
    "This quick test helps us:\n",
    "- Verify the NIM endpoint is working correctly\n",
    "- See the impact of in-context learning on a single example\n",
    "- Ensure our prompt engineering is effective\n",
    "- Identify any immediate issues before full evaluation\n",
    "\n",
    "The qualitative differences between zero-shot and few-shot responses here will inform our expectations for the quantitative evaluation that follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a4fe8-fdc9-4693-8f22-7cb7fa310b6c",
   "metadata": {},
   "source": [
    "When we finish the evaluation, we would want the results to be ingested to the mlflow server. To do that we've added a download and ingest function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87911c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "from time import sleep, time\n",
    "import warnings\n",
    "import zipfile\n",
    "\n",
    "import requests\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def download_and_process(eval_tag, eval_url, eval_id, mlflow_uri, polling_interval=10, timeout=6000):\n",
    "    \"\"\"\n",
    "    Poll an evaluation job, download its results archive, and ingest into MLflow.\n",
    "    Shows only essential status updates and progress.\n",
    "    \"\"\"\n",
    "    start_time = time()\n",
    "\n",
    "    # Monitor job status with progress updates\n",
    "    while True:\n",
    "        # Check for timeout\n",
    "        if time() - start_time > timeout:\n",
    "            raise RuntimeError(f\"Evaluation took more than {timeout} seconds.\")\n",
    "\n",
    "        # Get job status\n",
    "        job = requests.get(\n",
    "            f\"{eval_url}/v1/evaluation/jobs/{eval_id}\", timeout=10\n",
    "        ).json()\n",
    "        status = job.get(\"status\")\n",
    "        if isinstance(status, dict):\n",
    "            status = status.get(\"status\")\n",
    "\n",
    "        # Get progress if available\n",
    "        status_details = job.get(\"status_details\", {})\n",
    "        progress = status_details.get(\"progress\", 0)\n",
    "\n",
    "        # Break if job is done\n",
    "        if status not in {\"initializing\", \"running\", \"created\", \"pending\"}:\n",
    "            break\n",
    "\n",
    "        # Display minimal status\n",
    "        clear_output(wait=True)\n",
    "        elapsed = time() - start_time\n",
    "        print(f\"Status: {status} after {elapsed:.1f}s. Progress: {progress:.1f}%\")\n",
    "        sleep(polling_interval)\n",
    "\n",
    "    # Print final status\n",
    "    clear_output(wait=True)\n",
    "    print(\"Status details:\", status_details)\n",
    "\n",
    "    # Download and process results\n",
    "    url = f\"{eval_url}/v1/evaluation/jobs/{eval_id}/results\"\n",
    "    local_zip = f\"{eval_id}.zip\"\n",
    "    print(f\"Downloading results from: {url}\")\n",
    "\n",
    "    resp = requests.get(\n",
    "        url,\n",
    "        headers={\"accept\": \"application/json\"},\n",
    "        stream=True,\n",
    "        timeout=60,\n",
    "        verify=False,\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    # Create results directory structure\n",
    "    extract_dir = f\"{eval_id}_extracted\"\n",
    "    results_dir = os.path.join(extract_dir, \"results\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Save results directly as results.json\n",
    "    results_file = os.path.join(results_dir, \"results.json\")\n",
    "    with open(results_file, \"wb\") as fh:\n",
    "        for chunk in resp.iter_content(chunk_size=1 << 16):\n",
    "            if chunk:\n",
    "                fh.write(chunk)\n",
    "    print(f\"Saved results to: {results_file}\")\n",
    "\n",
    "    # MLflow ingestion\n",
    "    experiment_name = f\"{eval_tag}_{eval_id}\"\n",
    "    cmd = [\n",
    "        \"python3\",\n",
    "        \"integrations/MLFlow/mlflow_eval_integration.py\",\n",
    "        \"--results_abs_dir\",\n",
    "        os.path.abspath(results_dir),\n",
    "        \"--mlflow_uri\",\n",
    "        mlflow_uri,\n",
    "        \"--experiment_name\",\n",
    "        experiment_name,\n",
    "    ]\n",
    "    print(\"Running MLflow command:\\n\", \" \".join(cmd))\n",
    "\n",
    "    try:\n",
    "        out = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "        print(\"MLflow ingestion succeeded:\\n\", out.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        warnings.warn(\"MLflow ingestion failed:\\n\" + e.stderr)\n",
    "\n",
    "    return results_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20970a65",
   "metadata": {},
   "source": [
    "### Evaluation with NeMo Evaluator MS\n",
    "\n",
    "The NeMo Evaluator provides several ways to assess model performance:\n",
    "\n",
    "1. **Similarity Metrics** - Compare model outputs with ground truth using standard metrics\n",
    "2. **LLM-as-a-Judge** - Use a larger LLM to evaluate outputs\n",
    "3. Optional: **Academic Benchmarks** - Running evaluation using automatic benchmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a6055",
   "metadata": {},
   "source": [
    "Now we can evaluate and analyse the two different modes with the [NVIDIA NeMo Evaluator](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/overview.html). \n",
    "We can launch, monitor, and track results of evaluation jobs through a user-friendly API, to get insights into your model’s performance. \n",
    "\n",
    "Refer to [this](https://docs.nvidia.com/nemo/microservices/latest/api/evaluator.html) to learn more about the Evaluator API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad227cf2-8726-4ea7-b3c9-b618324842a6",
   "metadata": {},
   "source": [
    "#### Evaluation using Similarity Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca864c1",
   "metadata": {},
   "source": [
    "The NeMo Evaluator Similarity Metrics is a powerful tool for evaluating models on custom datasets by comparing the model's generated responses to ground truth responses. In this playbook, we will evaluate the two models for the title generation use case using Law Exchange's `test.jsonl` dataset. \n",
    "\n",
    "Custom evaluation gives results for following metrics:\n",
    "\n",
    "1. Accuracy :  \n",
    "\n",
    "    Measures the proportion of correct responses generated by the model, calculated as the ratio of the number of correct predictions to the total number of predictions. \n",
    "    \n",
    "    Best suited for tasks with a clear, discrete set of correct answers (e.g., classification or multiple-choice summarization tasks). Not effective for open-ended or creative outputs.\n",
    "\n",
    "1. Rouge-N : \n",
    "\n",
    "    Measures the overlap of n-grams (like bigrams, trigrams) between the generated text and reference texts. \n",
    "\n",
    "    Relevant for extractive summarization, where word overlap is a good indicator of content fidelity.\n",
    "\n",
    "1. BLEU (Bilingual Evaluation Understudy) : \n",
    "\n",
    "    Measures how closely the generated text resembles the ground truth by evaluating the overlap of n-grams, focusing on precision.\n",
    "\n",
    "    Suitable for machine translation or summarization tasks where linguistic structure alignment is more critical than semantic equivalence.\n",
    "\n",
    "1. EM (Exact Match) : \n",
    "\n",
    "    Measures the percentage of generated responses that exactly match the reference answers. \n",
    "\n",
    "    Ideal for tasks requiring verbatim reproduction or precise responses, such as classification and multiple choice question-answer use case.\n",
    "\n",
    "1. F1 : \n",
    "    \n",
    "    Combines precision (the proportion of correctly predicted words) and recall (the proportion of relevant ground truth words identified) into a harmonic mean. \n",
    "    \n",
    "    Suitable for tasks where partial correctness matters, such as abstractive summarization with some degree of variability in outputs.\n",
    "\n",
    "1. BERTScore : \n",
    "    \n",
    "    BERTScore uses a pre-trained BERT model to measure semantic similarity between generated and reference texts, comparing embeddings at the word and sentence levels. \n",
    "    \n",
    "    Ideal for abstractive summarization or generative tasks, where semantic alignment is more important than word-for-word overlap.\n",
    "\n",
    "These metrics are best suited for uses cases where the LLM generations use cases where model outputs are expected to closely align with a predefined ideal response. Specifically for our classification use case, we will test the fine-tuned model on BLEU and F1 Score.\n",
    "\n",
    "Refer to [this link](https://docs.nvidia.com/nemo/microservices/latest/evaluate/evaluation-types.html#similarity-metrics) to get more details on using the Nemo Evaluator on custom dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0fc383",
   "metadata": {},
   "source": [
    "#### Evaluation of zero-shot mode\n",
    "\n",
    "We can initialize our evaluation config, where we can choose to run inference on an `input_file` through NeMo Evaluator, and even point to a generated `output_file` containing the predictions. The `scorers` selected during eval launch are used to generate evaluation metrics. In our evaluation we will use online evaluation against the deployed NIM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040f4f51-66a3-40b8-973e-8ebd29c16ccc",
   "metadata": {},
   "source": [
    "Since both the Evaluator MS and the NIM are part of the same K8s cluster, we can't use the ingress to connect between the two microservices. We will use the microservice IP to reach the NIM in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e87e1e0-d48c-4670-be27-0bcc23a910b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://10.98.160.64:8000/v1/completions\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def get_service_ip(service_name, namespace=\"default\"):\n",
    "    # Run the kubectl command and get output in JSON format\n",
    "    cmd = f\"kubectl get svc {service_name} -n {namespace} -o json\"\n",
    "    result = subprocess.check_output(cmd, shell=True)\n",
    "    svc_json = json.loads(result)\n",
    "    return svc_json[\"spec\"][\"clusterIP\"]\n",
    "\n",
    "\n",
    "NIM_IP = get_service_ip(\"modeldeployment-meta-llama-3-1-8b-instruct-dgx-spark\")\n",
    "nim_k8s_url = f\"http://{NIM_IP}:8000/v1/completions\"\n",
    "print(nim_k8s_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac354940-9e0e-45f4-8ab1-5f170d7dad20",
   "metadata": {},
   "source": [
    "When we are preparing an evaluation job, we need to have an evaluation target, which represents the LLM endpoint that will be evaluated, and an evaluation configuration, which represents the type of evaluation that will be run, including all evaluation parameters. We will start by setting up the evaluation target with the NIM url. This will provide us with a unique ID that we will use later on when launching the evaluation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae801c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status: 409\n",
      "json: {'detail': 'Failed to create target: Error: EvaluationTarget default/NIM_llama_31_8b_dgx_spark already exists.'}\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "409 Client Error: Conflict for url: http://nemo.test/v1/evaluation/targets",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mstatus:\u001b[39m\u001b[33m\"\u001b[39m, resp.status_code)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mjson:\u001b[39m\u001b[33m\"\u001b[39m, resp.json())   \u001b[38;5;66;03m# show the error\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m NIM_model_target = resp.json()[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNIM evaluation target ID is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNIM_model_target\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/pyNeMo/lib/python3.11/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1021\u001b[39m     http_error_msg = (\n\u001b[32m   1022\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 409 Client Error: Conflict for url: http://nemo.test/v1/evaluation/targets"
     ]
    }
   ],
   "source": [
    "target_body = {\n",
    "    \"type\": \"model\",\n",
    "    \"name\": \"NIM_llama_31_8b_dgx_spark\",\n",
    "    \"namespace\": \"default\",\n",
    "    \"model\": {\"api_endpoint\": {\"url\": nim_k8s_url, \"model_id\": NIM_model_id}},\n",
    "}\n",
    "\n",
    "resp = requests.post(\n",
    "    f\"{eval_url}/v1/evaluation/targets\", json=target_body, verify=False\n",
    ")\n",
    "#print(\"status:\", resp.status_code)\n",
    "#print(\"json:\", resp.json())   # show the error\n",
    "#resp.raise_for_status()\n",
    "NIM_model_target = resp.json()[\"id\"]\n",
    "print(f\"NIM evaluation target ID is {NIM_model_target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23eec1d-3b30-40ea-8b8e-2aa05c062c4a",
   "metadata": {},
   "source": [
    "Setting up the evaluation configuration - pay attention to the files_url - it should be identical to the path in the DataStore microservice that you uploaded earlier in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54570b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 {'created_at': '2026-02-04T03:05:33.429245', 'updated_at': '2026-02-04T03:05:33.429246', 'name': 'nim_custom_similarity_v5', 'namespace': 'default', 'type': 'custom', 'params': {'parallelism': 1}, 'tasks': {'custom_similarity': {'type': 'chat-completion', 'params': {'template': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': '{{item.prompt}}'}], 'max_tokens': 50, 'temperature': 0.7, 'top_p': 1.0}}, 'metrics': {'bleu': {'type': 'bleu', 'params': {'references': ['{{item.reference_answer | trim}}']}}, 'f1': {'type': 'f1', 'params': {'ground_truth': '{{item.reference_answer | trim}}'}}}, 'dataset': {'schema_version': '1.0', 'id': 'dataset-66svM6ZKowFqy4NzaQhV4U', 'namespace': 'default', 'created_at': '2026-02-04T03:05:33.429093', 'updated_at': '2026-02-04T03:05:33.429095', 'custom_fields': {}, 'name': 'dataset-66svM6ZKowFqy4NzaQhV4U', 'version_id': 'main', 'version_tags': [], 'files_url': 'hf://datasets/default/legal_dataset_notebook_2/testing/testing.jsonl'}}}, 'id': 'eval-config-5PjWMfwrAykpEXKu3s18pt', 'custom_fields': {}}\n",
      "Evaluation configuration ID is eval-config-5PjWMfwrAykpEXKu3s18pt\n"
     ]
    }
   ],
   "source": [
    "config_body = {\n",
    "  \"type\": \"custom\",\n",
    "  \"name\": \"nim_custom_similarity_v5\",\n",
    "  \"namespace\": \"default\",\n",
    "  \"params\": {\"parallelism\": 1},\n",
    "  \"tasks\": {\n",
    "    \"custom_similarity\": {\n",
    "      \"type\": \"chat-completion\",\n",
    "      \"dataset\": {\n",
    "        \"files_url\": f\"hf://datasets/default/{dataset_name}/testing/testing.jsonl\"\n",
    "      },\n",
    "      \"params\": {\n",
    "        \"template\": {\n",
    "          \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"{{item.prompt}}\"}\n",
    "          ],\n",
    "          \"max_tokens\": 50,\n",
    "          \"temperature\": 0.7,\n",
    "          \"top_p\": 1.0\n",
    "        }\n",
    "      },\n",
    "      \"metrics\": {\n",
    "        \"bleu\": {\n",
    "          \"type\": \"bleu\",\n",
    "          \"params\": {\n",
    "            \"references\": [\"{{item.reference_answer | trim}}\"]\n",
    "          }\n",
    "        },\n",
    "        \"f1\": {\n",
    "          \"type\": \"f1\",\n",
    "          \"params\": {\n",
    "            \"ground_truth\": \"{{item.reference_answer | trim}}\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{eval_url}/v1/evaluation/configs\", json=config_body, verify=False)\n",
    "#print(resp.status_code, resp.json())\n",
    "#resp.raise_for_status()\n",
    "\n",
    "similarity_config_id = resp.json()[\"id\"]\n",
    "print(f\"Evaluation configuration ID is {similarity_config_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f30cbe-88d2-4f45-afbc-2852106a6ecf",
   "metadata": {},
   "source": [
    "Launching the evaluation job with the evaluation target ID and the evaluation configuration ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c5d92ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation job ID is eval-V8Xd82ey7zaNUmJZweXQeD\n"
     ]
    }
   ],
   "source": [
    "eval_tag = \"Llama-3.1-8B_dgx_spark_law_data_zero_shot\"\n",
    "eval_body = {\n",
    "    \"namespace\": \"default\",\n",
    "    \"config\": \"default/nim_custom_similarity_v5\",\n",
    "    \"target\": \"default/NIM_llama_31_8b_dgx_spark\",\n",
    "}\n",
    "resp = requests.post(f\"{eval_url}/v1/evaluation/jobs\", json=eval_body, verify=False)\n",
    "#print(\"status:\", resp.status_code)\n",
    "#print(\"json:\", resp.json())   # show the error\n",
    "#resp.raise_for_status()\n",
    "eval_id = resp.json()[\"id\"]\n",
    "print(f\"Evaluation job ID is {eval_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cbe35",
   "metadata": {},
   "source": [
    "Now we can poll the status until the evaluations finish. The evaluation can take a few minutes to an hour depending on the evaluation parameters, model performance, and size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2517183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status details: {'message': \"Task custom_similarity has failed due to templating error: 'dict object' has no attribute 'reference_answer'\", 'task_status': {'custom_similarity': 'failed'}, 'progress': 0.0, 'samples_processed': 0}\n",
      "Downloading results from: http://nemo.test/v1/evaluation/jobs/eval-2mGnFPYXT3Jk3tM3BLy9Gh/results\n",
      "Saved results to: eval-2mGnFPYXT3Jk3tM3BLy9Gh_extracted/results/results.json\n",
      "Running MLflow command:\n",
      " python3 integrations/MLFlow/mlflow_eval_integration.py --results_abs_dir /home/aaron/nemo-k8s-spark-dgx/nb/eval-2mGnFPYXT3Jk3tM3BLy9Gh_extracted/results --mlflow_uri http://192.168.49.2:30090 --experiment_name Llama-3.1-8B_dgx_spark_law_data_zero_shot_eval-2mGnFPYXT3Jk3tM3BLy9Gh\n",
      "MLflow ingestion succeeded:\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'eval-2mGnFPYXT3Jk3tM3BLy9Gh_extracted/results'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_and_process(eval_tag, eval_url, eval_id, mlflow_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c75e0",
   "metadata": {},
   "source": [
    "#### Evaluation of few-shots (ICL)\n",
    "\n",
    "We configure a similar evaluation job for the NIM with few-shots (ICL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d0bc94a-e3dd-48e5-b193-16ee3bd9b4ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      1\u001b[39m config_body = {\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msimilarity_metrics\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mnim_custom_similarity_icl\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     },\n\u001b[32m     15\u001b[39m }\n\u001b[32m     16\u001b[39m resp = requests.post(\n\u001b[32m     17\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/v1/evaluation/configs\u001b[39m\u001b[33m\"\u001b[39m, json=config_body, verify=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m similarity_icl_config_id = \u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluation configuration ID is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarity_icl_config_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'id'"
     ]
    }
   ],
   "source": [
    "config_body = {\n",
    "    \"type\": \"similarity_metrics\",\n",
    "    \"name\": \"nim_custom_similarity_icl\",\n",
    "    \"namespace\": \"default\",\n",
    "    \"params\": {\"max_tokens\": 50, \"temperature\": 0.7, \"extra\": {\"top_k\": 20}},\n",
    "    \"tasks\": {\n",
    "        \"custom_similarity_icl\": {\n",
    "            \"type\": \"default\",\n",
    "            \"dataset\": {\n",
    "                \"files_url\": f\"hf://datasets/default/{dataset_name}/testing/testing_icl.jsonl\"\n",
    "            },\n",
    "            \"metrics\": {\"bleu\": {\"type\": \"bleu\"}, \"f1\": {\"type\": \"f1\"}},\n",
    "        }\n",
    "    },\n",
    "}\n",
    "resp = requests.post(\n",
    "    f\"{eval_url}/v1/evaluation/configs\", json=config_body, verify=False\n",
    ")\n",
    "similarity_icl_config_id = resp.json()[\"id\"]\n",
    "print(f\"Evaluation configuration ID is {similarity_icl_config_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0968345-4f53-436d-930b-2afb2d2ee045",
   "metadata": {},
   "source": [
    "We can compare this performance to the zero-shot mode by launching a new evaluation job with the same evaluation target and the updated evaluation configuration that points to the ICL dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e72e7-924d-46b2-ae81-9aaf8823765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tag = \"Llama-3.2-3B_law_data_icl\"\n",
    "eval_body = {\n",
    "    \"namespace\": \"dafault\",\n",
    "    \"config\": \"default/nim_custom_similarity_icl\",\n",
    "    \"target\": \"default/NIM_llama_32_3b\",\n",
    "}\n",
    "resp = requests.post(f\"{eval_url}/v1/evaluation/jobs\", json=eval_body, verify=False)\n",
    "eval_id = resp.json()[\"id\"]\n",
    "print(f\"Evaluation job ID is {eval_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241ebf1d-d83a-4802-a448-b90186c6bedc",
   "metadata": {},
   "source": [
    "We can now monitor the progress of the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b04f4c-c267-4b2d-8f8d-8f649b99d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_process(eval_tag, eval_url, eval_id, mlflow_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08691b7-da38-4164-aa9d-f4d7cbbbbf3e",
   "metadata": {},
   "source": [
    "Now that you have the results for both evaluation, do you see any improvement coming from the ICL?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bd337d",
   "metadata": {},
   "source": [
    "### Evaluations with LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c625df5",
   "metadata": {},
   "source": [
    "With custom-LLM-as-a-Judge, an LLM can be evaluated by using another LLM as a judge. Nemo Evaluator follow a flexible and robust way to configure and customize llm-as-a-judge for your evaluation needs. Including (1) allowing users to bring in their own custom datasets, and (2) allowing any NIM model or OpenAI API end-point to be used as a judge model. \n",
    "\n",
    "LLM-as-a-Judge can be used for any use case, even highly generative ones, but the choice of judge is crucial in getting reliable evaluations. The judge model should have enough domain knowledge of the use case, compared to the model being evaluated, to be an effective judge. Generally, an LLM regarded as a high quality model should be used as the judge. \n",
    "\n",
    "Refer to Nemo Evaluator [docs](https://docs.nvidia.com/nemo/microservices/latest/evaluate/evaluation-custom.html#evaluation-with-llm-as-a-judge) to know more about LLM-as-a-Judge Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb0aaf4",
   "metadata": {},
   "source": [
    "We will go through following steps for using LLM-as-a-judge:\n",
    "1. Formatting data for evaluation\n",
    "1. Uploading custom dataset \n",
    "1. Setup an API endpoint for the Judge LLM\n",
    "1. Submitting evaluation job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7dc75a",
   "metadata": {},
   "source": [
    "#### Formatting data for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5393d4-4e24-4a45-bb4e-8946334d0ee8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "merged_rows = []\n",
    "for index, row in enumerate(inputs_dataset):\n",
    "    merged_row = {\n",
    "        \"question_id\": f\"summary_example_{index}\",\n",
    "        \"category\": row.get(\"category\", \"\"),\n",
    "        \"question\": row[\"prompt\"],  # The full consult or question text\n",
    "        \"ground_truth\": row[\n",
    "            \"completion\"\n",
    "        ],  # The reference/ground-truth summary or title\n",
    "    }\n",
    "    merged_rows.append(merged_row)\n",
    "\n",
    "# Limit data if needed\n",
    "limit_data = 10\n",
    "merged_dataset = Dataset.from_list(merged_rows[:limit_data])\n",
    "\n",
    "# Save to a single JSONL file\n",
    "merged_file = \"custom_dataset/eval_data.jsonl\"\n",
    "merged_dataset.to_json(merged_file)\n",
    "\n",
    "# Optional: print a sample row\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(merged_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2e73e",
   "metadata": {},
   "source": [
    "#### Uploading a Custom Dataset for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b99552",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = \"default/test-llm_as_a_judge\"\n",
    "repo_type = \"dataset\"\n",
    "local_file = \"./custom_dataset/eval_data.jsonl\"\n",
    "path_in_repo = \"eval_data.jsonl\"  # This will be the filename in the repo\n",
    "\n",
    "# Create the repo if it doesn't exist\n",
    "hf_api.create_repo(\n",
    "    repo_id=repo_name,\n",
    "    repo_type=repo_type,\n",
    "    exist_ok=True,  # Don't error if it already exists\n",
    ")\n",
    "\n",
    "# Upload the single file\n",
    "result = hf_api.upload_file(\n",
    "    path_or_fileobj=local_file,\n",
    "    path_in_repo=path_in_repo,\n",
    "    repo_id=repo_name,\n",
    "    repo_type=repo_type,\n",
    ")\n",
    "\n",
    "print(f\"File uploaded to: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e4505",
   "metadata": {},
   "source": [
    "#### Use a hosted endpoint as the judge LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2a262",
   "metadata": {},
   "source": [
    "To simplify the evaluation process, we will use a large LLM that is hosted in [build.nvidia.com](build.nvidia.com) to act as the judge LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd7363",
   "metadata": {},
   "source": [
    "Let's first test if the hosted NIM LLM endpoint is ready and available for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab8f20-5e48-4ebe-9fdb-009ec18790d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "# URL and headers as provided in the shell script\n",
    "judge_endpoint_url = BASE_URL\n",
    "judge_model = \"nvidia/llama-3.3-nemotron-super-49b-v1\"\n",
    "judge_api_token = NGC_API_KEY\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {judge_api_token}\",\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Data payload as a Python dictionary\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a limerick about the wonders of GPU computing.\",\n",
    "        }\n",
    "    ],\n",
    "    \"model\": judge_model,\n",
    "    \"top_p\": 0.7,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"seed\": 42,\n",
    "    \"stream\": False,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"temperature\": 0.2,\n",
    "}\n",
    "\n",
    "# Making the POST request\n",
    "response = requests.post(judge_endpoint_url, headers=headers, json=data)\n",
    "\n",
    "# Printing the response\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fac890",
   "metadata": {},
   "source": [
    "#### Submitting evaluation job using LLM-as-a-judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064047a",
   "metadata": {},
   "source": [
    "Now that we have judge LLM NIM deployed, and evaluation data uploaded in the Nemo Datastore, we will see how to use Nemo Evaluator API for submitting evaluation job using judge LLM.\n",
    "As our end-point is already defined, we only need to define the evaluation config in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2b663-8a3d-4643-8dda-1feb5feeef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# In order to use the workshop environment's proxy service (which is providing a valid API key for you) inside k8s pods, we need to use the proxy service's FQDN\n",
    "# which we set here. In your own environment, when providing your own API and not using the workshop's proxy service, you would set:\n",
    "# judge_endpoint_url = 'https://integrate.api.nvidia.com/v1/chat/completions'\n",
    "judge_endpoint_url = 'http://proxy.default.svc.cluster.local/v1/chat/completions'\n",
    "\n",
    "DATASET_URL = (\n",
    "    \"hf://datasets/default/test-llm_as_a_judge/\"  # or your actual dataset path\n",
    ")\n",
    "\n",
    "judge_eval_config = {\n",
    "    \"type\": \"custom\",\n",
    "    \"name\": \"custom_llm_as_a_judge\",\n",
    "    \"tasks\": {\n",
    "        \"law_summary\": {\n",
    "            \"type\": \"chat-completion\",\n",
    "            \"params\": {\n",
    "                \"template\": {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": (\n",
    "                                \"You are a headline generator for a legal Q&A forum. \"\n",
    "                                \"Return ONE concise, engaging title (max 15 words) that captures \"\n",
    "                                \"the core legal issue. \"\n",
    "                                \"Do NOT add quotes, labels, options, or extra text. \"\n",
    "                                \"Output exactly the title on a single line.\\n\"\n",
    "                            ),\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": (\"Question: {{ item.question }}\\n Title:\"),\n",
    "                        },\n",
    "                    ],\n",
    "                    \"max_tokens\": 200,\n",
    "                    \"temperature\": 0.0001,\n",
    "                }\n",
    "            },\n",
    "            \"dataset\": {\n",
    "                \"files_url\": \"hf://datasets/default/test-llm_as_a_judge/eval_data.jsonl\"\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"accuracy\": {\n",
    "                    \"type\": \"llm-judge\",\n",
    "                    \"params\": {\n",
    "                        \"model\": {\n",
    "                            \"api_endpoint\": {\n",
    "                                \"url\": judge_endpoint_url,\n",
    "                                \"model_id\": judge_model,\n",
    "                                \"api_key\": judge_api_token,\n",
    "                            },\n",
    "                            \"temperature\": 0.0001,\n",
    "                            \"limit_samples\": 5,\n",
    "                            \"parallelism\": 2,\n",
    "                        },\n",
    "                        \"template\": {\n",
    "                            \"messages\": [\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": (\n",
    "                                        \"You are a critic with expertise in judging summaries of articles. \"\n",
    "                                        \"You will be provided a source document and two summaries (Summary 1 and Summary 2).\\n\\n\"\n",
    "                                        \"on a scale from 1 to 5:\\n\"\n",
    "                                        \"- A higher rating (towards 5) indicates that Summary 1 is better than Summary 2.\\n\"\n",
    "                                        \"- A lower rating (towards 1) indicates Summary 2 is better than Summary 1.\\n\"\n",
    "                                        \"- A score of 3 would mean that both the summaries are of similar quality in all dimensions.\\n\"\n",
    "                                        \"Please respond with RATING: <number>\"\n",
    "                                    ),\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": (\n",
    "                                        \"Source Document: {{ item.question }}\\n\"\n",
    "                                        \"Summary 1: {{ item.ground_truth }}\\n\"\n",
    "                                        \"Summary 2: {{ sample.output_text }}\\n\"\n",
    "                                    ),\n",
    "                                },\n",
    "                            ]\n",
    "                        },\n",
    "                        \"scores\": {\n",
    "                            \"accuracy\": {\n",
    "                                \"type\": \"int\",\n",
    "                                \"parser\": {\n",
    "                                    \"type\": \"regex\",\n",
    "                                    \"pattern\": r\"RATING:\\s*(\\d+)\",\n",
    "                                },\n",
    "                            }\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                \"correctness\": {\n",
    "                    \"type\": \"llm-judge\",\n",
    "                    \"params\": {\n",
    "                        \"model\": {\n",
    "                            \"api_endpoint\": {\n",
    "                                \"url\": judge_endpoint_url,\n",
    "                                \"model_id\": judge_model,\n",
    "                                \"api_key\": judge_api_token,\n",
    "                            }\n",
    "                        },\n",
    "                        \"template\": {\n",
    "                            \"messages\": [\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": (\n",
    "                                        \"You are a judge. Rate the summary's correctness \"\n",
    "                                        \"(no false info) on a scale 1-5:\\n\"\n",
    "                                        \"1 = many inaccuracies … 5 = completely accurate\\n\"\n",
    "                                        \"Please respond with RATING: <number>\"\n",
    "                                    ),\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": (\n",
    "                                        \"Full Consult: {{ item.content }}\\n\\n\"\n",
    "                                        \"Summary: {{ sample.output_text }}\"\n",
    "                                    ),\n",
    "                                },\n",
    "                            ]\n",
    "                        },\n",
    "                        \"scores\": {\n",
    "                            \"correctness\": {\n",
    "                                \"type\": \"int\",\n",
    "                                \"parser\": {\n",
    "                                    \"type\": \"regex\",\n",
    "                                    \"pattern\": r\"RATING:\\s*(\\d+)\",\n",
    "                                },\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                \"conciseness\": {\n",
    "                    \"type\": \"llm-judge\",\n",
    "                    \"params\": {\n",
    "                        \"model\": {\n",
    "                            \"api_endpoint\": {\n",
    "                                \"url\": judge_endpoint_url,\n",
    "                                \"model_id\": judge_model,\n",
    "                                \"api_key\": judge_api_token,\n",
    "                            }\n",
    "                        },\n",
    "                        \"template\": {\n",
    "                            \"messages\": [\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": (\n",
    "                                        \"You are a judge. Rate the summary's conciseness \"\n",
    "                                        \"(no unimportant info) on a scale 1-5:\\n\"\n",
    "                                        \"1 = overly verbose … 5 = perfectly concise\\n\"\n",
    "                                        \"Please respond with RATING: <number>\"\n",
    "                                    ),\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": (\n",
    "                                        \"Full Consult: {{ item.content }}\\n\\n\"\n",
    "                                        \"Summary: {{ sample.output_text }}\"\n",
    "                                    ),\n",
    "                                },\n",
    "                            ]\n",
    "                        },\n",
    "                        \"scores\": {\n",
    "                            \"conciseness\": {\n",
    "                                \"type\": \"int\",\n",
    "                                \"parser\": {\n",
    "                                    \"type\": \"regex\",\n",
    "                                    \"pattern\": r\"RATING:\\s*(\\d+)\",\n",
    "                                },\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                \"readability\": {\n",
    "                    \"type\": \"llm-judge\",\n",
    "                    \"params\": {\n",
    "                        \"model\": {\n",
    "                            \"api_endpoint\": {\n",
    "                                \"url\": judge_endpoint_url,\n",
    "                                \"model_id\": judge_model,\n",
    "                                \"api_key\": judge_api_token,\n",
    "                            }\n",
    "                        },\n",
    "                        \"template\": {\n",
    "                            \"messages\": [\n",
    "                                {\n",
    "                                    \"role\": \"system\",\n",
    "                                    \"content\": (\n",
    "                                        \"You are a judge. Rate the summary's readability \"\n",
    "                                        \"(grammar & clarity) on a scale 1-5:\\n\"\n",
    "                                        \"1 = very hard to read … 5 = crystal clear\\n\"\n",
    "                                        \"Please respond with RATING: <number>\"\n",
    "                                    ),\n",
    "                                },\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": (\n",
    "                                        \"Question: {{ item.question }}\\n\\n\"\n",
    "                                        \"Summary: {{ sample.output_text }}\"\n",
    "                                    ),\n",
    "                                },\n",
    "                            ]\n",
    "                        },\n",
    "                        \"scores\": {\n",
    "                            \"readability\": {\n",
    "                                \"type\": \"int\",\n",
    "                                \"parser\": {\n",
    "                                    \"type\": \"regex\",\n",
    "                                    \"pattern\": r\"RATING:\\s*(\\d+)\",\n",
    "                                },\n",
    "                            },\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Submit the config to the NeMo Evaluator\n",
    "resp = requests.post(\n",
    "    f\"{eval_url}/v1/evaluation/configs\", json=judge_eval_config, verify=False\n",
    ")\n",
    "judge_llm_config_id = resp.json()[\"id\"]\n",
    "print(\"Evaluation config ID:\", judge_llm_config_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c25e5-d724-4508-adec-2ac696138606",
   "metadata": {},
   "source": [
    "Now that we have our evaluation configuration defined, we can launch the evaluation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39882132-5fc6-460e-8edc-8ee3092751ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIM_model_target = \"NIM_llama_32_3b\"  # or whatever your model target is\n",
    "\n",
    "eval_tag = \"llm_as_a_judge_zero_shot\"\n",
    "eval_body = {\n",
    "    \"target\": f\"default/{NIM_model_target}\",\n",
    "    \"config\": \"default/custom_llm_as_a_judge\",\n",
    "    \"tags\": [eval_tag],\n",
    "}\n",
    "\n",
    "resp = requests.post(\n",
    "    f\"{eval_url}/v1/evaluation/jobs\",\n",
    "    json=eval_body,\n",
    "    headers={\"accept\": \"application/json\"},\n",
    "    verify=False,\n",
    ")\n",
    "eval_id = resp.json()[\"id\"]\n",
    "print(\"Evaluation job ID:\", eval_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e49cc9-df7d-461a-8ef8-57bd653ed873",
   "metadata": {},
   "source": [
    "We can check the status of the evaluation job by pulling the status of the specific job or looking at the entire list of launched jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d5377-8f12-427b-b18c-0891deb3e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_process(eval_tag, eval_url, eval_id, mlflow_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce72eef",
   "metadata": {},
   "source": [
    "Let's look at the results! Remember from our LLM-As-A-Judge prompt:\n",
    "`You will evaluate the quality of Summary 2 on a scale of 1-5`\n",
    "\n",
    "More specifically, interpolate the scores between 1-5 based on the following rationale:\n",
    "- A higher rating (towards 5) indicates that Summary 1 is better than Summary 2.\n",
    "- A lower rating (towards 1) indicates Summary 2 is better than Summary 1.\n",
    "- A score of 3 would mean that both the summaries are of similar quality in all dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b936871-cd7a-486d-8b70-eb87367c4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def parse_eval_results(output):\n",
    "    # Navigate to the metrics dictionary\n",
    "    metrics = output[\"law_summary\"][\"metrics\"]\n",
    "    rows = []\n",
    "    for metric_name, metric_data in metrics.items():\n",
    "        # Each metric has a 'scores' dict, which may have a sub-metric (e.g., 'correctness', 'completeness', etc.)\n",
    "        for sub_metric, sub_metric_data in metric_data[\"scores\"].items():\n",
    "            value = sub_metric_data.get(\"value\")\n",
    "            stats = sub_metric_data.get(\"stats\", {})\n",
    "            row = {\n",
    "                \"metric\": metric_name,\n",
    "                \"value\": value,\n",
    "                \"count\": stats.get(\"count\"),\n",
    "                \"sum\": stats.get(\"sum\"),\n",
    "                \"mean\": stats.get(\"mean\"),\n",
    "            }\n",
    "            rows.append(row)\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7317138-5348-4671-ae9a-456f4642968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"{eval_url}/v1/evaluation/jobs/{eval_id}/results\"\n",
    "\n",
    "resp = requests.get(\n",
    "    url,\n",
    "    headers={\"accept\": \"application/json\"},\n",
    "    stream=True,\n",
    "    timeout=60,\n",
    "    verify=False,\n",
    ")\n",
    "\n",
    "metrics = resp.json()[\"tasks\"]\n",
    "\n",
    "print(parse_eval_results(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fdc2ad",
   "metadata": {},
   "source": [
    "##### You can now follow the same flow to evaluate with ICL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda946d",
   "metadata": {},
   "source": [
    "Finally, let's compare the evaluation scores for the two modes and see how it helps us decide which model better fits for our use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e0d425",
   "metadata": {},
   "source": [
    "Based on the results above, which mode will you take for your legal GenAI application? Is it enough? In the next notebook we will explore the impact of fine-tuning on the quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6ea47",
   "metadata": {},
   "source": [
    "# Optional - Other evaluation functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1676f90-27d8-46d5-804f-e101b810042e",
   "metadata": {},
   "source": [
    "Running one type of evaluation won't necessarily provide you with all the information you need to make a decision which model is the right one for your application. As LLMs may require to do multiple tasks, we will usually build a list of evaluation metrics we want to utilize to get a more granular score and better tools that will help us make a better decision. \n",
    "<br />\n",
    "We've added two additional ways that the Nemo Evaluator MS enables you to do exactly that. Give it a try and see if it provides you with the required information you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585549e3",
   "metadata": {},
   "source": [
    "### Evaluate with LM Evaluation Harness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e8559",
   "metadata": {},
   "source": [
    "[LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness/) supports over 60 standard academic benchmarks for LLMs, including MMLU, GSM8k, and hellaswag. Refer to [tasks](https://github.com/EleutherAI/lm-evaluation-harness/tree/v0.4.6/lm_eval/tasks#tasks) to see the list of tasks. The Nemo Evaluator microservice allows users to run these, all of which are accessible through the Nemo Evaluator API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd733893",
   "metadata": {},
   "source": [
    "The Nemo Evaluator microservice allows users to run these benchmarks through its API, streamlining the evaluation process for both researchers and practitioners in the field of natural language processing. Standard evaluation benchmarks, also known as Academic benchmarks, are useful for providing a baseline for general capabilities and identifying gaps in performance of a model. They provide a robust baseline for measuring general language understanding and comparing performance across models. For fine-tuned models, these benchmarks help ensure the model retains generalization while specializing in a particular domain. \n",
    "\n",
    "However, they often fail to capture domain-specific nuances critical for fine-tuned models, such as task precision or specialized terminology. To ensure relevance and effectiveness, fine-tuned models should also be evaluated using custom datasets tailored to their specific use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e32cf0",
   "metadata": {},
   "source": [
    "#### Evaluation of NIM with ifeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d9932d",
   "metadata": {},
   "source": [
    "Evaluating the ability of Large Language Models (LLMs) to follow natural language instructions is crucial, but current methods have limitations. Human evaluations are time-consuming and subjective, while automated evaluations using LLMs can be biased. To address this, researchers have introduced Instruction-Following Eval [IFEval](https://arxiv.org/pdf/2311.07911), a standardized benchmark that assesses LLMs' ability to follow \"verifiable instructions\", such as writing a certain number of words or mentioning specific keywords. The benchmark consists of 500 prompts with 25 types of verifiable instructions and provides a straightforward and reproducible way to evaluate LLMs. The researchers have made the code and data publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae53caf-5a70-4423-8d0b-780e27a7c6c0",
   "metadata": {},
   "source": [
    "We configure the new evaluation configuration for lm-eval `ifeval` task, you can use `limit` to limit the number of samples in the evaluation so the evaluation time will be shorter. -1 in `limit` means that the full evaluation will run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1223f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_body = {\n",
    "    \"type\": \"gsm8k\",\n",
    "    \"name\": \"my-configuration-lm-harness-gsm8k\",\n",
    "    \"namespace\": \"default\",\n",
    "    \"tasks\": {\"gsm8k_cot_llama\": {\"type\": \"gsm8k_cot_llama\"}},\n",
    "    \"params\": {\n",
    "        \"temperature\": 0.00001,\n",
    "        \"top_p\": 0.00001,\n",
    "        \"max_tokens\": 256,\n",
    "        \"stop\": [\"<|eot|>\"],\n",
    "        \"limit_samples\": 10,\n",
    "        \"extra\": {\n",
    "            \"num_fewshot\": 8,\n",
    "            \"batch_size\": 16,\n",
    "            \"bootstrap_iters\": 100000,\n",
    "            \"dataset_seed\": 42,\n",
    "            \"use_greedy\": True,\n",
    "            \"top_k\": 1,\n",
    "            \"hf_token\": \"<my-token>\",\n",
    "            \"tokenizer_backend\": \"hf\",\n",
    "            \"tokenizer\": \"utils/llama3-1-8b-instruct-tokenizer\",\n",
    "            \"apply_chat_template\": True,\n",
    "            \"fewshot_as_multiturn\": True\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "resp = requests.post(\n",
    "    f\"{eval_url}/v1/evaluation/configs\", json=config_body, verify=False\n",
    ")\n",
    "ifeval_config_id = resp.json()[\"id\"]\n",
    "print(ifeval_config_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b0edc",
   "metadata": {},
   "source": [
    "We have configured out our evaluation job and can submit it to our Nemo Evaluator endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bc7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tag = \"Llama-3.2-3B_ifeval\"\n",
    "eval_body = {\n",
    "    \"namespace\": \"default\",\n",
    "    \"tags\": [eval_tag],\n",
    "    \"target\": \"default/NIM_llama_32_3b\",\n",
    "    \"config\": \"default/my-configuration-lm-harness-gsm8k\",\n",
    "}\n",
    "\n",
    "resp = requests.post(f\"{eval_url}/v1/evaluation/jobs\", json=eval_body, verify=False)\n",
    "\n",
    "eval_id = resp.json()[\"id\"]\n",
    "print(f\"Evaluation job ID is {eval_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f49675",
   "metadata": {},
   "source": [
    "Now we can poll the status until the evaluations finish. The evaluation can take a few minutes to an hour depending on the evaluation parameters, model performance, and size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1aebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_process(eval_tag, eval_url, eval_id, mlflow_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04777a-bc21-410f-af39-2b870c9165d8",
   "metadata": {},
   "source": [
    "While the evaluation runs, try to look into the different tasks supported by lm-eval-harness repository and try answering the following questions - \n",
    " * If you want to evaluate your models on European languages (Spanish, Basque, French, ...), which tasks can help you do that?\n",
    " * While validating your model on non-English languages, how would you suggest approaching a new evaluation task? will you focus on translating existing tasks? why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyNeMo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
