{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#76b900\">**Notebook 3:** Retriever and RAG Evaluation </font>\n",
    "\n",
    "The rapid advancement of Large Language Models (LLMs) has led to a surge in their application to various natural language processing tasks, including information retrieval and question answering. However, the complexity and opacity of these models make it challenging to assess their performance and identify areas for improvement. \n",
    "<br />\n",
    "<br />\n",
    "Evaluating the retrieval and reranking capabilities of LLMs is crucial to ensure that they provide accurate and relevant results, particularly in high-stakes applications such as search engines, virtual assistants, and decision-support systems. By assessing the effectiveness of LLMs in retrieving and reranking relevant information, researchers and developers can identify potential biases, errors, and limitations, and develop more robust and reliable models that better serve the needs of users. Furthermore, evaluating retrieval and reranking LLMs can also inform the development of more efficient and effective training methods, leading to improved performance and faster convergence, which is essential for realizing the full potential of these powerful models.\n",
    "\n",
    "In the following notebook, we'll be exploring how to use [NeMo Evaluator microservice](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/overview.html) to evaluate [Retriever Models](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/models/models_retriever.html) as well as [Retrieval Augmented Generation (RAG) Models](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/models/models_rag.html)!\n",
    "\n",
    "We'll look at the following examples: \n",
    "\n",
    "- Retriever Model Evaluation on FiQA\n",
    "- Retriever + Reranking Evaluation on FiQA\n",
    "- Bonus - Retrieval Augmented Generation (RAG) Evaluation on FiQA with Ragas Metrics (HW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Set-up and Notebook Dependencies\n",
    "\n",
    "In order to run this notebook, the following will need to be up and running: \n",
    "\n",
    "- Evaluator Microservice, which can be conveniently deployed through the [Deploying with Helm](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/deploy-helm.html) guide\n",
    "- NVIDIA NIM Text Embedding or a hosted Text Embedding model, `nvidia/llama-3.2-nv-embedqa-1b-v2`, which can be deployed using this [Getting Started](https://docs.nvidia.com/nim/nemo-retriever/text-embedding/latest/getting-started.html) guide\n",
    "- NVIDIA NIM Text Reranking or a hosted Text Reranking model, `nvidia/llama-3.2-nv-rerankqa-1b-v2`, which can be deployed using this [Getting Started](https://docs.nvidia.com/nim/nemo-retriever/text-reranking/latest/getting-started.html) guide\n",
    "- NVIDIA NIM for LLM, `meta/llama-3.1-8b-instruct` (the same one we used in Notebook 2), which can be deployed using this [Getting Started](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html) guide\n",
    "\n",
    "Once all of our services are up and running, we can install the Python `requests` library, which we will use to communicate with the Evaluator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU requests huggingface_hub==0.26.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to provide the Evaluation API URL in the cell below.\n",
    "\n",
    "> NOTE: Your evaluation URL will be provided as part of your deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_URL = \"http://nemo-evaluator.local\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to provide the endpoints for your model addresses and model names, which will be set-up as part of the deployment process for each NIM.\n",
    "\n",
    "Below is an example of the default value for the embedding NIM:\n",
    "\n",
    "- Embedding: \n",
    "  - EMBEDDING_URL: `http://localhost:8000/v1/embeddings`\n",
    "  - EMBEDDING_MODEL_NAME: `nvidia/nv-embedqa-e5-v5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\n",
      "meta-llama3-1-8b-instruct   ClusterIP   10.102.195.57   <none>        8000/TCP   3d4h\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n llama3-1-8b-instruct get svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'http://10.102.195.57:8000/v1/completions'\n"
     ]
    }
   ],
   "source": [
    "# LLM\n",
    "NIM_IP = \"10.102.195.57\" #FIXME with the IP generated above\n",
    "LLM_URL = f\"http://{NIM_IP}:8000/v1/completions\"\n",
    "LLM_MODEL_NAME = \"meta/llama-3.1-8b-instruct\"\n",
    "pp(LLM_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "EMBEDDING_URL = \"https://integrate.api.nvidia.com/v1/embeddings\"\n",
    "EMBEDDING_MODEL_NAME = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "\n",
    "# reranker \n",
    "RERANKER_URL = \"https://ai.api.nvidia.com/v1/retrieval/nvidia/llama-3_2-nv-rerankqa-1b-v2/reranking\"\n",
    "RERANKER_MODEL_NAME = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can verify our Evaluation API is up and running with the built-in health check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib3\n",
    "from pprint import pp\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "resp = requests.get(f\"{EVAL_URL}/health\", verify=False)\n",
    "pp(resp.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGC_API_KEY='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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Model Evaluation on FiQA\n",
    "\n",
    "For our first evaluation, we're going to evaluate our Retrieval Model (`nvidia/llama-3.2-nv-embedqa-1b-v2`) on the [FiQA](https://sites.google.com/view/fiqa/) retrieval task as part of the [BeIR](https://github.com/beir-cellar/beir) benchmark.\n",
    "\n",
    "The core pieces we need to provide are: \n",
    "\n",
    "- `top_k`, how many documents to retriever through our retriever model\n",
    "- `query_embedding_url`, the address of your hosted `nvidia/llama-3.2-nv-embedqa-1b-v2` endpoint.\n",
    "- `query_embedding_model`, this will be `nvidia/llama-3.2-nv-embedqa-1b-v2` if you're following the notebook exactly.\n",
    "- `index_embedding_url`, which will mirror the `query_embedding_url` assuming that you're using the same NIM deployment for both Query Embedding and Index embedding.\n",
    "- `index_embedding_model`, this will mirror the `query_embedding_model` assuming that you're using the same NIM deployment for both Query Embedding and Index embedding.\n",
    "\n",
    "> NOTE: While it's possible to use different NIM *deployments* for Query/Index Embedding - you will need to ensure the underlying model is the same between both.\n",
    "\n",
    "We'll also want to ensure we've set-up our evaluations correctly by following the available [documentation](https://developer.nvidia.com/docs/nemo-microservices/evaluation/source/evaluations/evaluations_retriever.html) for Retriever evaluations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set up the evalutor API end points - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_endpoint = f\"{EVAL_URL}/v1/evaluation/targets\"\n",
    "eval_config_endpoint = f\"{EVAL_URL}/v1/evaluation/configs\"\n",
    "job_endpoint = f\"{EVAL_URL}/v1/evaluation/jobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_target_config = {\n",
    " \"type\": \"retriever\",\n",
    " \"retriever\": {\n",
    "   \"pipeline\": {\n",
    "     \"query_embedding_model\": {\n",
    "       \"api_endpoint\": {\n",
    "           \"url\": EMBEDDING_URL,\n",
    "           \"model_id\": EMBEDDING_MODEL_NAME,\n",
    "           \"api_key\": NGC_API_KEY\n",
    "       }\n",
    "     },\n",
    "     \"index_embedding_model\": {\n",
    "       \"api_endpoint\": {\n",
    "           \"url\": EMBEDDING_URL,\n",
    "           \"model_id\": EMBEDDING_MODEL_NAME,\n",
    "           \"api_key\": NGC_API_KEY\n",
    "       }\n",
    "     },\n",
    "     \"top_k\": 5\n",
    "   }\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first take the embedding model for a quick test-drive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='0.0.0.0', port=8000): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8c1eeec610>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/urllib3/connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/urllib3/connection.py:445\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/urllib3/connection.py:276\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/urllib3/connection.py:213\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    217\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f8c1eeec610>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='0.0.0.0', port=8000): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8c1eeec610>: Failed to establish a new connection: [Errno 111] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello NGC 2025\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: EMBEDDING_MODEL_NAME,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m }\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Making the POST request\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEMBEDDING_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Printing the response\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/requests/adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='0.0.0.0', port=8000): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8c1eeec610>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "EMBEDDING_URL = 'http://0.0.0.0:8000'\n",
    "\n",
    "headers = {\n",
    "    # 'Authorization': f'Bearer {NGC_API_KEY}',\n",
    "    'Accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Data payload as a Python dictionary\n",
    "data = {\n",
    "    \"input\": [\"Hello NGC 2025\"],\n",
    "    \"model\": EMBEDDING_MODEL_NAME,\n",
    "    \"input_type\": \"query\"\n",
    "}\n",
    "\n",
    "# Making the POST request\n",
    "response = requests.post(EMBEDDING_URL, headers=headers, json=data)\n",
    "\n",
    "# Printing the response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are clear to fire off the request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval-target-L5AUA4EGoN8AKtMjS7yfMp\n"
     ]
    }
   ],
   "source": [
    "retriever_response = requests.post(\n",
    "    target_endpoint,\n",
    "    json=retriever_target_config,\n",
    "    headers={'accept': 'application/json'},\n",
    "    verify=False)\n",
    "\n",
    "retriever_target_name = retriever_response.json()[\"name\"]\n",
    "print(retriever_target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll capture our target ID for the coming steps - but with this step we have created our target and are ready to create an evaluation configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Name: eval-target-LyUYbe3QD3r1Yn6MBgZYmT, Target Namespace: -\n"
     ]
    }
   ],
   "source": [
    "retriever_target_namespace = retriever_response.json()[\"namespace\"]\n",
    "print(f\"Target Name: {retriever_target_name}, Target Namespace: {retriever_target_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab our evaluation configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_eval_config = {\n",
    " \"type\": \"retriever\",\n",
    " \"tasks\": [\n",
    "   {\n",
    "     \"type\": \"beir\",\n",
    "     \"dataset\": {\n",
    "       \"format\": \"beir\",\n",
    "       \"files_url\": \"fiqa\"\n",
    "     },\n",
    "     \"metrics\": [\n",
    "       {\n",
    "         \"name\": \"recall_5\",\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"ndcg_cut_5\",\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"recall_10\",\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"ndcg_cut_10\",\n",
    "       }\n",
    "     ]\n",
    "   }\n",
    " ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our payload - we can send it to our Nemo Evaluator endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval-config-CbrMyHq1VAwcv3MAykACAS\n"
     ]
    }
   ],
   "source": [
    "retriever_eval_response = requests.post(\n",
    "    eval_config_endpoint,\n",
    "    json=retriever_eval_config,\n",
    "    headers={'accept': 'application/json'},\n",
    "    verify=False)\n",
    "\n",
    "retriever_config_name = retriever_eval_response.json()[\"name\"]\n",
    "print(retriever_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again capture our evaluation config for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Name: eval-config-CbrMyHq1VAwcv3MAykACAS, Config Namespace: default\n"
     ]
    }
   ],
   "source": [
    "retriever_config_namespace = retriever_eval_response.json()[\"namespace\"]\n",
    "print(f\"Config Name: {retriever_config_name}, Config Namespace: {retriever_config_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an Evaluation Job\n",
    "\n",
    "Now that we have our `target_id` and `config_id` -  we have everything we need to run an evaluation.\n",
    "\n",
    "Let's see the process to create and run a job! \n",
    "\n",
    "First things first, we need to create a job payload to send to our endpoint - this will point to our target, and our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = {\n",
    "    \"target\": f\"default/{retriever_target_name}\",\n",
    "    \"config\": f\"default/{retriever_config_name}\",\n",
    "    \"tags\": [\n",
    "        \"embedding-fiqa\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is fire off our job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: eval-JaSyG6LemDDz4PmUHPY56A\n"
     ]
    }
   ],
   "source": [
    "retriever_job_response = requests.post(\n",
    "    job_endpoint,\n",
    "    json=job_config,\n",
    "    headers={'accept': 'application/json'},\n",
    "    verify=False)\n",
    "\n",
    "retriever_job_id = retriever_job_response.json()[\"id\"]\n",
    "print(f\"Job ID: {retriever_job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring\n",
    "\n",
    "We can monitor the status of our job through the following endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = \"initializing\"\n",
    "\n",
    "while status == \"running\" or status == \"initializing\":\n",
    "    sleep(120)\n",
    "    resp = requests.get(f\"{EVAL_URL}/v1/evaluation/jobs/{retriever_job_id}\")\n",
    "    status = resp.json()[\"status\"][\"status\"]\n",
    "pp(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on the status of our evaluation in the cell below. \n",
    "\n",
    "> NOTE: When the evaluation `status` becomes `succeeded`, the `evaluation_results` field will become populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded the file successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The URL we're sending the GET request to\n",
    "url = f\"{EVAL_URL}/v1/evaluation/jobs/-/{retriever_job_id}/download-results\"\n",
    "filename = f\"retriever_{retriever_job_id}.zip\"\n",
    "# Additional headers being sent with the request\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "}\n",
    "\n",
    "# Since you're using -k in curl, it allows connections to SSL sites without certificates.\n",
    "# In requests, you can achieve this by setting verify to False.\n",
    "# WARNING: This is insecure and should only be used with caution.\n",
    "response = requests.get(url, headers=headers, verify=False)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Write the content of the response to a file\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Downloaded the file successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever + Reranking Evaluation on FiQA\n",
    "\n",
    "For our second evaluation, we're going to evaluate our Retrieval Model (`nvidia/llama-3.2-nv-embedqa-1b-v2`) on the [FiQA](https://sites.google.com/view/fiqa/) retrieval task as part of the [BeIR](https://github.com/beir-cellar/beir) benchmark.\n",
    "\n",
    "Instead of simply using a Retriever model, however, this example will also leverage a Reranking model (`nvidia/llama-3.2-nv-rerankqa-1b-v2`) to rerank the retrieved results.\n",
    "\n",
    "We'll rerun the same evaluation configuration as we did above - with a few extra parameters in our `retriever` configuration:\n",
    "\n",
    "- `ranker_url`, which will point to our reranking model\n",
    "- `ranker_model`, which will contain the name of our reranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"rankings\":[{\"index\":0,\"logit\":1.564453125},{\"index\":3,\"logit\":-0.497802734375},{\"index\":2,\"logit\":-3.697265625},{\"index\":1,\"logit\":-6.2578125}],\"usage\":{\"prompt_tokens\":220,\"total_tokens\":220}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {NGC_API_KEY}',\n",
    "    'Accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Data payload as a Python dictionary\n",
    "data = {\n",
    "    \"model\": RERANKER_MODEL_NAME,\n",
    "    \"query\": {\"text\": \"which way did the traveler go?\"},\n",
    "    \"passages\": [\n",
    "        {\"text\": \"two roads diverged in a yellow wood, and sorry i could not travel both and be one traveler, long i stood and looked down one as far as i could to where it bent in the undergrowth;\"},\n",
    "        {\"text\": \"then took the other, as just as fair, and having perhaps the better claim because it was grassy and wanted wear, though as for that the passing there had worn them really about the same,\"},\n",
    "        {\"text\": \"and both that morning equally lay in leaves no step had trodden black. oh, i marked the first for another day! yet knowing how way leads on to way i doubted if i should ever come back.\"},\n",
    "        {\"text\": \"i shall be telling this with a sigh somewhere ages and ages hense: two roads diverged in a wood, and i, i took the one less traveled by, and that has made all the difference.\"}\n",
    "    ],\n",
    "    \"truncate\": \"END\"\n",
    "}\n",
    "\n",
    "# Making the POST request\n",
    "response = requests.post(RERANKER_URL, headers=headers, json=data)\n",
    "\n",
    "# Printing the response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_target_config = {\n",
    " \"type\": \"retriever\",\n",
    " \"retriever\": {\n",
    "   \"pipeline\": {\n",
    "     \"query_embedding_model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": EMBEDDING_URL,\n",
    "         \"model_id\": EMBEDDING_MODEL_NAME,\n",
    "           \"api_key\": NGC_API_KEY\n",
    "       }\n",
    "     },\n",
    "     \"index_embedding_model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": EMBEDDING_URL,\n",
    "         \"model_id\": EMBEDDING_MODEL_NAME,\n",
    "        \"api_key\": NGC_API_KEY\n",
    "       }\n",
    "     },\n",
    "     \"reranker_model\": {\n",
    "       \"api_endpoint\": {\n",
    "         \"url\": RERANKER_URL,\n",
    "         \"model_id\":RERANKER_MODEL_NAME,\n",
    "        \"api_key\": NGC_API_KEY\n",
    "       }\n",
    "     },\n",
    "     \"top_k\": 10\n",
    "   }\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are clear to fire off the request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval-target-MWrthRkE2tkJ8jC1WgsZ7x\n"
     ]
    }
   ],
   "source": [
    "reranker_response = requests.post(\n",
    "    target_endpoint,\n",
    "    json=reranker_target_config,\n",
    "    headers={'accept': 'application/json'},\n",
    "    verify=False)\n",
    "\n",
    "reranker_target_name = reranker_response.json()[\"name\"]\n",
    "print(reranker_target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll capture our target ID for the coming steps - but with this step we have created our target and are ready to create an evaluation configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Name: eval-target-MWrthRkE2tkJ8jC1WgsZ7x, Target Namespace: -\n"
     ]
    }
   ],
   "source": [
    "reranker_target_namespace = reranker_response.json()[\"namespace\"]\n",
    "print(f\"Target Name: {reranker_target_name}, Target Namespace: {reranker_target_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our payload - we can send it to our Nemo Evaluator endpoint.\n",
    "\n",
    "> NOTE: Notice how we don't have to re-create our evaluation configuration since we already created it for the Embedding model evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an Evaluation Job\n",
    "\n",
    "Now that we have our `target_id` and `config_id` -  we have everything we need to run an evaluation.\n",
    "\n",
    "Let's see the process to create and run a job! \n",
    "\n",
    "First things first, we need to create a job payload to send to our endpoint - this will point to our target, and our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker_job_config = {\n",
    "    \"target\": f\"default/{reranker_target_name}\",\n",
    "    \"config\": f\"default/{retriever_config_name}\",\n",
    "    \"tags\": [\n",
    "        \"embedding-rerank-fiqa\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is fire off our job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: eval-Dw5SrTXA3wQsuwiMgiw6gX\n"
     ]
    }
   ],
   "source": [
    "reranker_job_response = requests.post(\n",
    "    job_endpoint,\n",
    "    json=reranker_job_config,\n",
    "    headers={'accept': 'application/json'},\n",
    "    verify=False)\n",
    "\n",
    "reranker_job_id = reranker_job_response.json()[\"id\"]\n",
    "print(f\"Job ID: {reranker_job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring\n",
    "\n",
    "We can monitor the status of our job through the following endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': '-',\n",
      " 'name': 'eval-Dw5SrTXA3wQsuwiMgiw6gX',\n",
      " 'tags': ['embedding-rerank-fiqa'],\n",
      " 'id': 'eval-Dw5SrTXA3wQsuwiMgiw6gX',\n",
      " 'target': {'namespace': '-',\n",
      "            'name': 'eval-target-MWrthRkE2tkJ8jC1WgsZ7x',\n",
      "            'type': 'retriever',\n",
      "            'model': None,\n",
      "            'retriever': {'pipeline': {'query_embedding_model': {'api_endpoint': {'url': 'https://integrate.api.nvidia.com/v1/embeddings',\n",
      "                                                                                  'model_id': 'nvidia/llama-3.2-nv-embedqa-1b-v2',\n",
      "                                                                                  'api_key': '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'},\n",
      "                                                                 'cached_outputs': None},\n",
      "                                       'index_embedding_model': {'api_endpoint': {'url': 'https://integrate.api.nvidia.com/v1/embeddings',\n",
      "                                                                                  'model_id': 'nvidia/llama-3.2-nv-embedqa-1b-v2',\n",
      "                                                                                  'api_key': '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'},\n",
      "                                                                 'cached_outputs': None},\n",
      "                                       'reranker_model': {'api_endpoint': {'url': 'https://ai.api.nvidia.com/v1/retrieval/nvidia/llama-3_2-nv-rerankqa-1b-v2/reranking',\n",
      "                                                                           'model_id': 'nvidia/llama-3.2-nv-rerankqa-1b-v2',\n",
      "                                                                           'api_key': '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'},\n",
      "                                                          'cached_outputs': None},\n",
      "                                       'top_k': 10},\n",
      "                          'cached_outputs': None},\n",
      "            'rag': None,\n",
      "            'tags': None,\n",
      "            'id': 'eval-target-MWrthRkE2tkJ8jC1WgsZ7x'},\n",
      " 'config': {'id': 'eval-config-RcJvhd5vcRVKdDH9X3F4t8',\n",
      "            'namespace': '-',\n",
      "            'name': 'eval-config-RcJvhd5vcRVKdDH9X3F4t8',\n",
      "            'type': 'retriever',\n",
      "            'tags': [],\n",
      "            'params': None,\n",
      "            'tasks': [{'type': 'beir',\n",
      "                       'dataset': {'files_url': 'fiqa', 'format': 'beir'},\n",
      "                       'metrics': [{'name': 'recall_5', 'params': None},\n",
      "                                   {'name': 'ndcg_cut_5', 'params': None},\n",
      "                                   {'name': 'recall_10', 'params': None},\n",
      "                                   {'name': 'ndcg_cut_10', 'params': None}]}],\n",
      "            'aggregate_metrics': None},\n",
      " 'status': {'name': 'evaluation',\n",
      "            'level': 'evaluation',\n",
      "            'status': 'unavailable',\n",
      "            'message': None,\n",
      "            'jobs': [{'job_id': None,\n",
      "                      'status': 'failed',\n",
      "                      'job_name': 'retriever.automatic.beir.beir'},\n",
      "                     {'job_id': None,\n",
      "                      'status': 'succeeded',\n",
      "                      'job_name': 'retriever.automatic.beir.cleanup_milvus'}],\n",
      "            'children': [{'name': 'beir',\n",
      "                          'level': 'benchmark',\n",
      "                          'status': 'partial_success',\n",
      "                          'message': None,\n",
      "                          'jobs': [{'job_id': None,\n",
      "                                    'status': 'failed',\n",
      "                                    'job_name': 'retriever.automatic.beir.beir'},\n",
      "                                   {'job_id': None,\n",
      "                                    'status': 'succeeded',\n",
      "                                    'job_name': 'retriever.automatic.beir.cleanup_milvus'}],\n",
      "                          'children': [{'name': 'beir',\n",
      "                                        'level': 'task',\n",
      "                                        'status': 'failed',\n",
      "                                        'message': None,\n",
      "                                        'jobs': [{'job_id': None,\n",
      "                                                  'status': 'failed',\n",
      "                                                  'job_name': 'retriever.automatic.beir.beir'}],\n",
      "                                        'children': []},\n",
      "                                       {'name': 'cleanup_milvus',\n",
      "                                        'level': 'task',\n",
      "                                        'status': 'succeeded',\n",
      "                                        'message': None,\n",
      "                                        'jobs': [{'job_id': None,\n",
      "                                                  'status': 'succeeded',\n",
      "                                                  'job_name': 'retriever.automatic.beir.cleanup_milvus'}],\n",
      "                                        'children': []}]}]},\n",
      " 'created_at': '2025-02-04T14:51:53Z',\n",
      " 'results': []}\n"
     ]
    }
   ],
   "source": [
    "status = \"initializing\"\n",
    "\n",
    "while status == \"running\" or status == \"initializing\":\n",
    "    sleep(120)\n",
    "    resp = requests.get(f\"{EVAL_URL}/v1/evaluation/jobs/{reranker_job_id}\")\n",
    "    status = resp.json()[\"status\"][\"status\"]\n",
    "pp(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check on the status of our evaluation in the cell below. \n",
    "\n",
    "> NOTE: When the evaluation `status` becomes `succeeded`, the `evaluation_results` field will become populated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it's done - let's look at the full results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': '-', 'name': 'eval-Dw5SrTXA3wQsuwiMgiw6gX', 'tags': ['embedding-rerank-fiqa'], 'id': 'eval-Dw5SrTXA3wQsuwiMgiw6gX', 'target': {'namespace': '-', 'name': 'eval-target-MWrthRkE2tkJ8jC1WgsZ7x', 'type': 'retriever', 'model': None, 'retriever': {'pipeline': {'query_embedding_model': {'api_endpoint': {'url': 'https://integrate.api.nvidia.com/v1/embeddings', 'model_id': 'nvidia/llama-3.2-nv-embedqa-1b-v2', 'api_key': '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'}, 'cached_outputs': None}, 'index_embedding_model': {'api_endpoint': {'url': 'https://integrate.api.nvidia.com/v1/embeddings', 'model_id': 'nvidia/llama-3.2-nv-embedqa-1b-v2', 'api_key': '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'}, 'cached_outputs': None}, 'reranker_model': {'api_endpoint': {'url': 'https://ai.api.nvidia.com/v1/retrieval/nvidia/llama-3_2-nv-rerankqa-1b-v2/reranking', 'model_id': 'nvidia/llama-3.2-nv-rerankqa-1b-v2', 'api_key': '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'}, 'cached_outputs': None}, 'top_k': 10}, 'cached_outputs': None}, 'rag': None, 'tags': None, 'id': 'eval-target-MWrthRkE2tkJ8jC1WgsZ7x'}, 'config': {'id': 'eval-config-RcJvhd5vcRVKdDH9X3F4t8', 'namespace': '-', 'name': 'eval-config-RcJvhd5vcRVKdDH9X3F4t8', 'type': 'retriever', 'tags': [], 'params': None, 'tasks': [{'type': 'beir', 'dataset': {'files_url': 'fiqa', 'format': 'beir'}, 'metrics': [{'name': 'recall_5', 'params': None}, {'name': 'ndcg_cut_5', 'params': None}, {'name': 'recall_10', 'params': None}, {'name': 'ndcg_cut_10', 'params': None}]}], 'aggregate_metrics': None}, 'status': {'name': 'evaluation', 'level': 'evaluation', 'status': 'unavailable', 'message': None, 'jobs': [{'job_id': None, 'status': 'failed', 'job_name': 'retriever.automatic.beir.beir'}, {'job_id': None, 'status': 'succeeded', 'job_name': 'retriever.automatic.beir.cleanup_milvus'}], 'children': [{'name': 'beir', 'level': 'benchmark', 'status': 'partial_success', 'message': None, 'jobs': [{'job_id': None, 'status': 'failed', 'job_name': 'retriever.automatic.beir.beir'}, {'job_id': None, 'status': 'succeeded', 'job_name': 'retriever.automatic.beir.cleanup_milvus'}], 'children': [{'name': 'beir', 'level': 'task', 'status': 'failed', 'message': None, 'jobs': [{'job_id': None, 'status': 'failed', 'job_name': 'retriever.automatic.beir.beir'}], 'children': []}, {'name': 'cleanup_milvus', 'level': 'task', 'status': 'succeeded', 'message': None, 'jobs': [{'job_id': None, 'status': 'succeeded', 'job_name': 'retriever.automatic.beir.cleanup_milvus'}], 'children': []}]}]}, 'created_at': '2025-02-04T14:51:53Z', 'results': []}\n"
     ]
    }
   ],
   "source": [
    "print(reranker_monitoring_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `evaluation_results` field will contain our `metrics` along with their name, and their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded the file successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The URL we're sending the GET request to\n",
    "url = f\"{EVAL_URL}/v1/evaluation/jobs/-/{reranker_job_id}/download-results\"\n",
    "filename = f\"reranker_{reranker_job_id}.zip\"\n",
    "# Additional headers being sent with the request\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "}\n",
    "\n",
    "# Since you're using -k in curl, it allows connections to SSL sites without certificates.\n",
    "# In requests, you can achieve this by setting verify to False.\n",
    "# WARNING: This is insecure and should only be used with caution.\n",
    "response = requests.get(url, headers=headers, verify=False)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Write the content of the response to a file\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Downloaded the file successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 1 - Retrieval Augmented Generation (RAG) Evaluation on FIQA with Ragas Metrics\n",
    "\n",
    "With the most recent release of NeMo Evaluator microservice, not only can we evaluate Retrievers and Rerankers - we can also Evaluate RAG!\n",
    "\n",
    "Once again, we're going to evaluate on the [FiQA](https://sites.google.com/view/fiqa/) retrieval task as part of the [BeIR](https://github.com/beir-cellar/beir) benchmark.\n",
    "\n",
    "We're also going to evaluate our RAG pipeline on the [Ragas](https://docs.ragas.io/en/stable/howtos/index.html) metrics [\"Faithfulness\"](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html). This can be done by extending our evaluation configuration in the following ways:\n",
    "\n",
    "1. We can create the model type `rag`, and provide our `retriever` configuration we used in the first evaluation.\n",
    "2. We need to provide a `context_ordering` parameter, in this case we'll use `desc` which will order our context in descending score.\n",
    "3. We need to provide a \"generator\" (LLM) that can be used to generate responses based on the retrieved context!\n",
    "\n",
    "We'll also need to add in a number of `judge_` parameters to help calculate the Faithfulness metric.\n",
    "\n",
    "Let's look at an example evaluation configuration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {NGC_API_KEY}',\n",
    "    'Accept': 'application/json',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Data payload as a Python dictionary\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a limerick about the wonders of GPU computing.\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": LLM_MODEL_NAME,\n",
    "    \"top_p\": 0.7,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"seed\": 42,\n",
    "    \"stream\": False,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"temperature\": 0.2\n",
    "}\n",
    "\n",
    "# Making the POST request\n",
    "response = requests.post(LLM_URL, headers=headers, json=data)\n",
    "\n",
    "# Printing the response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_target_config = {\n",
    " \"type\": \"rag\",\n",
    " \"rag\": {\n",
    "   \"pipeline\": {\n",
    "     \"retriever\": {\n",
    "       \"pipeline\": {\n",
    "         \"query_embedding_model\": {\n",
    "           \"api_endpoint\": {\n",
    "               \"url\": EMBEDDING_URL,\n",
    "               \"model_id\": EMBEDDING_MODEL_NAME,\n",
    "               \"api_key\": NGC_API_KEY\n",
    "               \n",
    "           }\n",
    "         },\n",
    "         \"index_embedding_model\": {\n",
    "           \"api_endpoint\": {\n",
    "               \"url\": EMBEDDING_URL,\n",
    "               \"model_id\": EMBEDDING_MODEL_NAME,\n",
    "               \"api_key\": NGC_API_KEY\n",
    "           }\n",
    "         }\n",
    "       }\n",
    "     },\n",
    "     \"model\": {\n",
    "       \"api_endpoint\": {\n",
    "           \"url\": LLM_URL,\n",
    "           \"model_id\": LLM_MODEL_NAME,\n",
    "           \"api_key\": NGC_API_KEY\n",
    "       }\n",
    "     }\n",
    "   }\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to point our request at the `v1/evaluation/targets` endpoint to create the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are clear to fire off the request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval-target-3HUyYMYJ4F1Taskr6sbfdc\n"
     ]
    }
   ],
   "source": [
    "rag_response = requests.post(\n",
    "    target_endpoint,\n",
    "    json=rag_target_config,\n",
    "    headers={'accept': 'application/json'},\n",
    "    verify=False)\n",
    "\n",
    "rag_target_name = rag_response.json()[\"name\"]\n",
    "print(rag_target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll capture our target ID for the coming steps - but with this step we have created our target and are ready to create an evaluation configuration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Name: eval-target-3HUyYMYJ4F1Taskr6sbfdc, Target Namespace: -\n"
     ]
    }
   ],
   "source": [
    "rag_target_namespace = rag_response.json()[\"namespace\"]\n",
    "print(f\"Target Name: {rag_target_name}, Target Namespace: {rag_target_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab our evaluation configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_config = {\n",
    " \"type\": \"rag\",\n",
    " \"tasks\": [\n",
    "   {\n",
    "     \"type\": \"beir\",\n",
    "     \"params\": {\n",
    "       \"judge_llm\": {\n",
    "         \"api_endpoint\": {\n",
    "           \"url\": LLM_URL,\n",
    "           \"model_id\": LLM_MODEL_NAME,\n",
    "             \"api_key\": NGC_API_KEY\n",
    "         }\n",
    "       },\n",
    "       \"judge_embeddings\": {\n",
    "         \"api_endpoint\": {\n",
    "           \"url\": EMBEDDING_URL,\n",
    "           \"model_id\": EMBEDDING_MODEL_NAME,\n",
    "             \"api_key\": NGC_API_KEY\n",
    "         }\n",
    "       },\n",
    "       \"judge_timeout\": 300,\n",
    "       \"judge_max_retries\": 5,\n",
    "       \"judge_max_workers\": 16\n",
    "     },\n",
    "     \"dataset\": {\n",
    "       \"files_url\": \"nfcorpus\",\n",
    "       \"format\": \"beir\"\n",
    "     },\n",
    "     \"metrics\": [\n",
    "       {\n",
    "         \"name\": \"recall_5\"\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"ndcg_cut_5\"\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"recall_10\"\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"ndcg_cut_10\"\n",
    "       },\n",
    "       {\n",
    "         \"name\": \"faithfulness\"\n",
    "       }\n",
    "     ]\n",
    "   }\n",
    " ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our payload - we can send it to our Nemo Evaluator endpoint.\n",
    "\n",
    "We'll set up our Evaluator endpoint URL..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval-config-DPAvbz89MTTqjX4G8yQ9P6\n"
     ]
    }
   ],
   "source": [
    "rag_eval_response = requests.post(\n",
    "    eval_config_endpoint,\n",
    "    json=rag_eval_config,\n",
    "    headers={'accept': 'application/json'},\n",
    "    verify=False)\n",
    "\n",
    "rag_config_name = rag_eval_response.json()[\"name\"]\n",
    "print(rag_config_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again capture our evaluation config for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Name: eval-config-DPAvbz89MTTqjX4G8yQ9P6, Config Namespace: -\n"
     ]
    }
   ],
   "source": [
    "rag_config_namespace = rag_eval_response.json()[\"namespace\"]\n",
    "print(f\"Config Name: {rag_config_name}, Config Namespace: {rag_config_namespace}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an Evaluation Job\n",
    "\n",
    "Now that we have our `target_id` and `config_id` -  we have everything we need to run an evaluation.\n",
    "\n",
    "Let's see the process to create and run a job! \n",
    "\n",
    "First things first, we need to create a job payload to send to our endpoint - this will point to our target, and our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_job_config = {\n",
    "    \"target\": f\"default/{rag_target_name},\n",
    "    \"config\": f\"ddefault/{rag_config_name},\n",
    "    \"tags\": [\n",
    "        \"rag-eval\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's left to do is fire off our job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: eval-75pK5iNYWdPNnyaer2yVGB\n"
     ]
    }
   ],
   "source": [
    "rag_job_response = requests.post(\n",
    "    job_endpoint,\n",
    "    json=rag_job_config,\n",
    "    headers={'accept': 'application/json'},\n",
    "    verify=False)\n",
    "\n",
    "rag_job_id = rag_job_response.json()[\"id\"]\n",
    "print(f\"Job ID: {rag_job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring\n",
    "\n",
    "We can monitor the status of our job through the following endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'namespace': '-',\n",
      " 'name': 'eval-75pK5iNYWdPNnyaer2yVGB',\n",
      " 'tags': ['rag-eval'],\n",
      " 'id': 'eval-75pK5iNYWdPNnyaer2yVGB',\n",
      " 'target': {'namespace': '-',\n",
      "            'name': 'eval-target-3HUyYMYJ4F1Taskr6sbfdc',\n",
      "            'type': 'rag',\n",
      "            'model': None,\n",
      "            'retriever': None,\n",
      "            'rag': {'pipeline': {'retriever': {'pipeline': {'query_embedding_model': {'api_endpoint': {'url': 'https://integrate.api.nvidia.com/v1/embeddings',\n",
      "                                                                                                       'model_id': 'nvidia/llama-3.2-nv-embedqa-1b-v2',\n",
      "                                                                                                       'api_key': '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'},\n",
      "                                                                                      'cached_outputs': None},\n",
      "                                                            'index_embedding_model': {'api_endpoint': {'url': 'https://integrate.api.nvidia.com/v1/embeddings',\n",
      "                                                                                                       'model_id': 'nvidia/llama-3.2-nv-embedqa-1b-v2',\n",
      "                                                                                                       'api_key': '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'},\n",
      "                                                                                      'cached_outputs': None},\n",
      "                                                            'reranker_model': None,\n",
      "                                                            'top_k': 10},\n",
      "                                               'cached_outputs': None},\n",
      "                                 'model': {'api_endpoint': {'url': 'https://integrate.api.nvidia.com/v1/chat/completions',\n",
      "                                                            'model_id': 'meta/llama-3.3-70b-instruct',\n",
      "                                                            'api_key': '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'},\n",
      "                                           'cached_outputs': None},\n",
      "                                 'context_ordering': 'desc'},\n",
      "                    'cached_outputs': None},\n",
      "            'tags': None,\n",
      "            'id': 'eval-target-3HUyYMYJ4F1Taskr6sbfdc'},\n",
      " 'config': {'id': 'eval-config-DPAvbz89MTTqjX4G8yQ9P6',\n",
      "            'namespace': '-',\n",
      "            'name': 'eval-config-DPAvbz89MTTqjX4G8yQ9P6',\n",
      "            'type': 'rag',\n",
      "            'tags': [],\n",
      "            'params': None,\n",
      "            'tasks': [{'type': 'beir',\n",
      "                       'dataset': {'files_url': 'nfcorpus', 'format': 'beir'},\n",
      "                       'metrics': [{'name': 'recall_5', 'params': None},\n",
      "                                   {'name': 'ndcg_cut_5', 'params': None},\n",
      "                                   {'name': 'recall_10', 'params': None},\n",
      "                                   {'name': 'ndcg_cut_10', 'params': None},\n",
      "                                   {'name': 'faithfulness', 'params': None}],\n",
      "                       'params': {'judge_llm': {'api_endpoint': {'url': 'https://integrate.api.nvidia.com/v1/chat/completions',\n",
      "                                                                 'model_id': 'meta/llama-3.3-70b-instruct'},\n",
      "                                                'cached_outputs': None},\n",
      "                                  'judge_embeddings': {'api_endpoint': {'url': 'https://integrate.api.nvidia.com/v1/embeddings',\n",
      "                                                                        'model_id': 'nvidia/llama-3.2-nv-embedqa-1b-v2'},\n",
      "                                                       'cached_outputs': None},\n",
      "                                  'judge_timeout': 300,\n",
      "                                  'judge_max_retries': 5,\n",
      "                                  'judge_max_workers': 16}}],\n",
      "            'aggregate_metrics': None},\n",
      " 'status': {'name': 'evaluation',\n",
      "            'level': 'evaluation',\n",
      "            'status': 'unavailable',\n",
      "            'message': None,\n",
      "            'jobs': [{'job_id': None,\n",
      "                      'status': 'succeeded',\n",
      "                      'job_name': 'rag.automatic.rag.beir'},\n",
      "                     {'job_id': None,\n",
      "                      'status': 'succeeded',\n",
      "                      'job_name': 'rag.automatic.rag.evaluation'},\n",
      "                     {'job_id': None,\n",
      "                      'status': 'succeeded',\n",
      "                      'job_name': 'rag.automatic.rag.generation'}],\n",
      "            'children': [{'name': 'rag',\n",
      "                          'level': 'benchmark',\n",
      "                          'status': 'succeeded',\n",
      "                          'message': None,\n",
      "                          'jobs': [{'job_id': None,\n",
      "                                    'status': 'succeeded',\n",
      "                                    'job_name': 'rag.automatic.rag.beir'},\n",
      "                                   {'job_id': None,\n",
      "                                    'status': 'succeeded',\n",
      "                                    'job_name': 'rag.automatic.rag.evaluation'},\n",
      "                                   {'job_id': None,\n",
      "                                    'status': 'succeeded',\n",
      "                                    'job_name': 'rag.automatic.rag.generation'}],\n",
      "                          'children': [{'name': 'beir',\n",
      "                                        'level': 'task',\n",
      "                                        'status': 'succeeded',\n",
      "                                        'message': None,\n",
      "                                        'jobs': [{'job_id': None,\n",
      "                                                  'status': 'succeeded',\n",
      "                                                  'job_name': 'rag.automatic.rag.beir'}],\n",
      "                                        'children': []},\n",
      "                                       {'name': 'evaluation',\n",
      "                                        'level': 'task',\n",
      "                                        'status': 'succeeded',\n",
      "                                        'message': None,\n",
      "                                        'jobs': [{'job_id': None,\n",
      "                                                  'status': 'succeeded',\n",
      "                                                  'job_name': 'rag.automatic.rag.evaluation'}],\n",
      "                                        'children': []},\n",
      "                                       {'name': 'generation',\n",
      "                                        'level': 'task',\n",
      "                                        'status': 'succeeded',\n",
      "                                        'message': None,\n",
      "                                        'jobs': [{'job_id': None,\n",
      "                                                  'status': 'succeeded',\n",
      "                                                  'job_name': 'rag.automatic.rag.generation'}],\n",
      "                                        'children': []}]}]},\n",
      " 'created_at': '2025-02-04T14:54:35Z',\n",
      " 'results': []}\n"
     ]
    }
   ],
   "source": [
    "status = \"initializing\"\n",
    "\n",
    "while status == \"running\" or status == \"initializing\":\n",
    "    sleep(120)\n",
    "    resp = requests.get(f\"{EVAL_URL}/v1/evaluation/jobs/{rag_job_id}\")\n",
    "    status = resp.json()[\"status\"][\"status\"]\n",
    "pp(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded the file successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The URL we're sending the GET request to\n",
    "url = f\"{EVAL_URL}/v1/evaluation/jobs/-/{rag_job_id}/download-results\"\n",
    "filename = f\"rag_{rag_job_id}.zip\"\n",
    "# Additional headers being sent with the request\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "}\n",
    "\n",
    "# Since you're using -k in curl, it allows connections to SSL sites without certificates.\n",
    "# In requests, you can achieve this by setting verify to False.\n",
    "# WARNING: This is insecure and should only be used with caution.\n",
    "response = requests.get(url, headers=headers, verify=False)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Write the content of the response to a file\n",
    "    with open(filename, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Downloaded the file successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above evaluations provide you with the initial tools to understand the quality of your RAG pipeline. You can try and modify the models to see how it impacts the quality of the results. Additionally, you can also build a custom dataset and use it instead of the default datasets used in the above evaluation tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
